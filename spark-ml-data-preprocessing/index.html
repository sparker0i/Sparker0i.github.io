<!DOCTYPE html>
<html lang="en" class="auto-color">
<head>

    <title>Spark ML: Numeric and Text Data Preprocessing</title>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="HandheldFriendly" content="True" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    
    <link rel="preload" as="style" href="/assets/built/screen.css?v=fe10ead0d3" />
    <link rel="preload" as="script" href="/assets/built/casper.js?v=fe10ead0d3" />

    <link rel="stylesheet" type="text/css" href="/assets/built/screen.css?v=fe10ead0d3" />

    <meta name="description" content="Preprocessing deals with detecting &amp; correcting inaccurate records from data. Learn how you can take a step to preprocess Big data with Spark ML.">
    <link rel="icon" href="http://localhost:2368/content/images/size/w256h256/2020/02/Untitled.png" type="image/png">
    <link rel="canonical" href="http://localhost:2368/spark-ml-data-preprocessing/">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <link rel="amphtml" href="http://localhost:2368/spark-ml-data-preprocessing/amp/">
    
    <meta property="og:site_name" content="Sparker0i&#x27;s Blog">
    <meta property="og:type" content="article">
    <meta property="og:title" content="Spark ML: Numeric and Text Data Preprocessing">
    <meta property="og:description" content="Data preprocessing is the process of detecting and correcting inaccurate records from a dataset. Its importance is more paramount when it comes to big data. Learn how you can preprocess data in Spark ML.">
    <meta property="og:url" content="http://localhost:2368/spark-ml-data-preprocessing/">
    <meta property="og:image" content="http://localhost:2368/content/images/2020/05/mllib.jpg">
    <meta property="article:published_time" content="2020-06-06T08:02:00.000Z">
    <meta property="article:modified_time" content="2020-06-06T08:02:00.000Z">
    <meta property="article:tag" content="Spark">
    <meta property="article:tag" content="Spark ML">
    
    <meta property="article:publisher" content="https://www.facebook.com/Sparker0i">
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="Spark ML: Numeric and Text Data Preprocessing">
    <meta name="twitter:description" content="Data preprocessing is the process of detecting and correcting inaccurate records from a dataset. Its importance is more paramount when it comes to big data. Learn how you can preprocess data in Spark ML.">
    <meta name="twitter:url" content="http://localhost:2368/spark-ml-data-preprocessing/">
    <meta name="twitter:image" content="http://localhost:2368/content/images/2020/05/mllib.jpg">
    <meta name="twitter:label1" content="Written by">
    <meta name="twitter:data1" content="Aaditya Menon">
    <meta name="twitter:label2" content="Filed under">
    <meta name="twitter:data2" content="Spark, Spark ML">
    <meta name="twitter:site" content="@Sparker0i">
    <meta name="twitter:creator" content="@Sparker0i">
    <meta property="og:image:width" content="600">
    <meta property="og:image:height" content="315">
    
    <script type="application/ld+json">
{
    "@context": "https://schema.org",
    "@type": "Article",
    "publisher": {
        "@type": "Organization",
        "name": "Sparker0i&#x27;s Blog",
        "url": "http://localhost:2368/",
        "logo": {
            "@type": "ImageObject",
            "url": "http://localhost:2368/content/images/2020/02/6006221719b070975f272141125c0689.png"
        }
    },
    "author": {
        "@type": "Person",
        "name": "Aaditya Menon",
        "image": {
            "@type": "ImageObject",
            "url": "http://localhost:2368/content/images/2022/02/72678158_2843214719045409_4986419922138562560_n.jpg",
            "width": 2000,
            "height": 2000
        },
        "url": "http://localhost:2368/author/sparker0i/",
        "sameAs": [
            "https://twitter.com/Sparker0i"
        ]
    },
    "headline": "Spark ML: Numeric and Text Data Preprocessing",
    "url": "http://localhost:2368/spark-ml-data-preprocessing/",
    "datePublished": "2020-06-06T08:02:00.000Z",
    "dateModified": "2020-06-06T08:02:00.000Z",
    "image": {
        "@type": "ImageObject",
        "url": "http://localhost:2368/content/images/2020/05/mllib.jpg",
        "width": 600,
        "height": 315
    },
    "keywords": "Spark, Spark ML",
    "description": "Data preprocessing is the process of detecting and correcting inaccurate records from a dataset. Its importance is more paramount when it comes to big data. Learn how you can preprocess data in Spark ML.",
    "mainEntityOfPage": "http://localhost:2368/spark-ml-data-preprocessing/"
}
    </script>

    <meta name="generator" content="Ghost 5.54">
    <link rel="alternate" type="application/rss+xml" title="Sparker0i&#x27;s Blog" href="http://localhost:2368/rss/">
    
    <script defer src="https://cdn.jsdelivr.net/ghost/sodo-search@~1.1/umd/sodo-search.min.js" data-key="4315ec64cd8859e309a18fe281" data-styles="https://cdn.jsdelivr.net/ghost/sodo-search@~1.1/umd/main.css" data-sodo-search="http://localhost:2368/" crossorigin="anonymous"></script>
    
    <link href="http://localhost:2368/webmentions/receive/" rel="webmention">
    <script defer src="/public/cards.min.js?v=fe10ead0d3"></script>
    <link rel="stylesheet" type="text/css" href="/public/cards.min.css?v=fe10ead0d3">
    <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-7XJJR5L4E3"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-7XJJR5L4E3');
</script>
<meta property="fb:pages" content="110085507196751" />
<meta name="google-site-verification" content="dW-FKUoBj2jzKbsORQiymBPtiZ3ROGD5gseblJwYl80" />
<script data-ad-client="ca-pub-7053772126133454" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><style>:root {--ghost-accent-color: #000000;}</style>

</head>
<body class="post-template tag-spark tag-spark-ml is-head-left-logo has-sans-body has-cover">
<div class="viewport">

    <header id="gh-head" class="gh-head outer">
        <div class="gh-head-inner inner">
            <div class="gh-head-brand">
                <a class="gh-head-logo" href="http://localhost:2368">
                        <img src="http://localhost:2368/content/images/2020/02/6006221719b070975f272141125c0689.png" alt="Sparker0i&#x27;s Blog">
                </a>
                <button class="gh-search gh-icon-btn" aria-label="Search this site" data-ghost-search><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke="currentColor" stroke-width="2" width="20" height="20"><path stroke-linecap="round" stroke-linejoin="round" d="M21 21l-6-6m2-5a7 7 0 11-14 0 7 7 0 0114 0z"></path></svg></button>
                <button class="gh-burger"></button>
            </div>

            <nav class="gh-head-menu">
                <ul class="nav">
    <li class="nav-scala"><a href="http://localhost:2368/tag/scala/">Scala</a></li>
    <li class="nav-spark"><a href="http://localhost:2368/tag/spark/">Spark</a></li>
</ul>

            </nav>

            <div class="gh-head-actions">
                        <button class="gh-search gh-icon-btn" data-ghost-search><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke="currentColor" stroke-width="2" width="20" height="20"><path stroke-linecap="round" stroke-linejoin="round" d="M21 21l-6-6m2-5a7 7 0 11-14 0 7 7 0 0114 0z"></path></svg></button>
            </div>
        </div>
    </header>

    <div class="site-content">
        



<main id="site-main" class="site-main">
<article class="article post tag-spark tag-spark-ml ">

    <header class="article-header gh-canvas">

        <div class="article-tag post-card-tags">
                <span class="post-card-primary-tag">
                    <a href="/tag/spark/">Spark</a>
                </span>
        </div>

        <h1 class="article-title">Cool Spark ML - Part 2: Preprocessing of Data</h1>

            <p class="article-excerpt">Data preprocessing is the process of detecting and correcting inaccurate records from a dataset. Its importance is more paramount when it comes to big data. Learn how you can preprocess data in Spark ML.</p>

        <div class="article-byline">
        <section class="article-byline-content">

            <ul class="author-list">
                <li class="author-list-item">
                    <a href="/author/sparker0i/" class="author-avatar">
                        <img class="author-profile-image" src="/content/images/size/w100/2022/02/72678158_2843214719045409_4986419922138562560_n.jpg" alt="Aaditya Menon" />
                    </a>
                </li>
            </ul>

            <div class="article-byline-meta">
                <h4 class="author-name"><a href="/author/sparker0i/">Aaditya Menon</a></h4>
                <div class="byline-meta-content">
                    <time class="byline-meta-date" datetime="2020-06-06">Jun 6, 2020</time>
                        <span class="byline-reading-time"><span class="bull">&bull;</span> 10 min read</span>
                </div>
            </div>

        </section>
        </div>

            <figure class="article-image">
                <img
                    srcset="/content/images/size/w300/2020/05/mllib.jpg 300w,
                            /content/images/size/w600/2020/05/mllib.jpg 600w,
                            /content/images/size/w1000/2020/05/mllib.jpg 1000w,
                            /content/images/size/w2000/2020/05/mllib.jpg 2000w"
                    sizes="(min-width: 1400px) 1400px, 92vw"
                    src="/content/images/size/w2000/2020/05/mllib.jpg"
                    alt="Cool Spark ML - Part 2: Preprocessing of Data"
                />
            </figure>

    </header>

    <section class="gh-content gh-canvas">
        <p><em>Note: This is the second article of the series: Cool Spark ML. The other parts can be found below:</em></p><ul><li><a href="http://localhost:2368/spark-machine-learning-knn/">Part 1: K Nearest Neighbours</a></li><li>Part 2: Preprocessing of Data (current)</li></ul><p>People who have been performing Machine Learning for quite a long time know that Data Preprocessing is a key step before running any algorithms on the data. In a majority of datasets, you might always find null, or incomplete values. The data would also be inconsistent across columns, which directly affects algorithms using distance measures.</p><p>This is where Data Preprocessing comes in. It is a crucial step which involves cleaning and organizing the data to make it suitable for building models. In other words, if you don't perform Preprocessing, your models may not be accurate.</p><p>While there are quite a lot of articles online about Data Preprocessing in Python, there aren't a lot of them in Spark, or even Scala. In this post, I will be dealing with the ways you can perform Data Preprocessing in Spark on Scala.</p><p><em>PS. You might be asking why I'm dealing with this now when I have actually written KNN in Spark before. The truth is, KNN isn't officially supported inside Spark ML module. What I wrote in the previous article was a top-to-bottom version of KNN performed using Spark. You can also say that I'm doing a complete reset of this series 😅</em></p><p>Just like always, the codes for all posts in this series will be available on <a href="https://github.com/Sparker0i/Cool-Spark-ML?ref=localhost">my GitHub repo</a>.</p><h2 id="types-of-preprocessing-in-spark">Types of Preprocessing in Spark</h2><p>There are two types of preprocessing:</p><ul><li>Numeric Data</li><li>Text Data</li></ul><h2 id="numeric-data">Numeric Data</h2><figure class="kg-card kg-image-card"><img src="http://localhost:2368/content/images/2020/05/1_GAcEj37smCOCZMrqp-rjjA.png" class="kg-image" alt loading="lazy"></figure><p>There are three ways you can preprocess numeric data in Spark:</p><ul><li>Normalize</li><li>Standardize</li><li>Bucketize</li></ul><p>To illustrate Normalize and Standardize, I'll be using some Scala magic which will generate my points as a Vector. Each vector represents a point in a 3-Dimensional Space.</p><figure class="kg-card kg-code-card"><pre><code class="language-scala">val points = for (i &lt;- 1 to 1000) yield (i, Vectors.dense(
    Array(
        (math.random * (10 - 1)) * i + 1.0,
        (math.random * (10000 - 1000)) + 1000.0,
        math.random * i
    )
))

val featuresDf = points.toDF("id", "features")</code></pre><figcaption>Creating a Random Set of Points in a 3D space</figcaption></figure><p>Doing the above results in the following <code>DataFrame</code>:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://localhost:2368/content/images/2020/05/image-2.png" class="kg-image" alt loading="lazy"><figcaption>Each element inside Features column represents a point in a 3-D space.</figcaption></figure><h3 id="normalize">Normalize</h3><p>Normalization is the process of mapping numeric data from their original range into a range of 0 to 1. The lowest value of the original range gets value of 0, and the highest gets the value 1. All the other values in the original range will fall between these two. </p><p>This is important because there may be multiple attributes with different ranges. <em>E.g. Salary values may range between 3 and 8+ digit numbers, years in company will be between 1- and 2-digit numbers. </em>The reason we want to normalize those attributes in a <code>[0,1]</code> range is so that when algorithms that use distance as a measure, they don't weigh some attributes like salary more heavily than others.</p><p>The formula to convert values in an un-normalized column to a normalized form is given by:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://localhost:2368/content/images/2020/05/image-5.png" class="kg-image" alt loading="lazy"><figcaption>Normalization Formula</figcaption></figure><p>Where:</p><ul><li><code>x</code> is the value inside a column to be normalized, </li><li><code>x(new)</code> is the normalized value,</li><li><code>x(min)</code> is the minimum value of that column, and</li><li><code>x(max)</code> is the maximum value of that column</li></ul><p>Working on the <code>featuresDf</code> created above, we will import <code>MinMaxScaler</code> from the <code>org.apache.spark.ml.feature</code> package. We now have to create an instance of the <code>MinMaxScaler</code>. It will take two parameters: Input column name, and an Output Column name. This object will transform the contents of the input column vectors into a scaled version, and save it into the output column.</p><p>In our case, we will be using our <code>features</code> column inside <code>featuresDf</code> as the input column, and our output column will be named <code>sFeatures</code>. We create the instance in this manner:</p><figure class="kg-card kg-code-card"><pre><code class="language-scala">val featureScaler = new MinMaxScaler()
    .setInputCol("features")
    .setOutputCol("sfeatures")</code></pre><figcaption>Creating the <code>MinMaxScaler</code> object.</figcaption></figure><p>Next, we have to <code>fit</code> the data present in our <code>featuresDf</code> inside this <code>featureScaler</code> and later <code>transform</code> to create the scaled data. This is done using the code below:</p><figure class="kg-card kg-code-card"><pre><code class="language-scala">val scaledDf = featureScaler.fit(featuresDf)
	.transform(featuresDf)</code></pre><figcaption>Transforming original values into normalized ones</figcaption></figure><p>Now, if we have a look at our transformed data:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://localhost:2368/content/images/2020/05/image-4.png" class="kg-image" alt loading="lazy"><figcaption>Normalized <code>DataFrame</code></figcaption></figure><p>You can then use this new <code>sFeatures</code> to calculate distances among points.</p><h3 id="standardize">Standardize</h3><p>Now, we may have data whose values can be mapped to a bell-shaped curve, or normally distributed but maybe not exactly. With standardization, we map our data and transform it, which has a variance of 1 and/or a mean value of 0. This is done because some machine learning algorithms, like SVM, work better this way.</p><p>Thus, what happens is when we apply standardization, our data is slightly shifted in its shape so that it becomes more normalized, or more like a bell curve. The formula to convert values in a non-standardized column to a standardized form is given by:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://localhost:2368/content/images/2020/05/image-6.png" class="kg-image" alt loading="lazy"><figcaption>Standardization Formula</figcaption></figure><p>Where:</p><ul><li><code>x</code> is the value to be standardized</li><li><code>x(new)</code> is the standardized value</li><li><code>μ</code> is the mean of the column</li><li><code>σ</code> is the standard deviation of the column.</li></ul><p>Again, we will be using the <code>featuresDf</code> created above. We will import <code>StandardScaler</code> from the <code>org.apache.spark.ml.feature</code> package. Just like <code>MinMaxScaler</code>, an instance of <code>StandardScaler</code> will require an input column and an output column. In our case, we will still continue with <code>features</code> and <code>sFeatures</code>. We will then <code>fit</code> the data inside the scaler and later <code>transform</code> the data. I've combined both these steps into a single code snippet:</p><figure class="kg-card kg-code-card"><pre><code class="language-scala">val featureStandardScaler = new StandardScaler()
    .setInputCol("features")
    .setOutputCol("sfeatures")
    .setWithStd(true)
    .setWithMean(true)
    
val standardizedDf = featureStandardScaler.fit(featuresDf)
    .transform(featuresDf)</code></pre><figcaption>Standardization of numeric data in Spark ML</figcaption></figure><p>Now if we have a look at our transformed data:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://localhost:2368/content/images/2020/05/image-8.png" class="kg-image" alt loading="lazy"><figcaption>Standardized Numeric Data</figcaption></figure><p>Wait, weren't the values supposed to be scaled within the range of <code>[-1, 1]</code>? Well, that's the surprise associated with the <code>StandardScaler</code>. It uses the unbiased sample standard deviation instead of the population standard deviation. </p><p>In other words, while the standard deviation will be 1 (or very close to 1), the mean may not be necessarily 0. To scale your data in a way that the range of numbers is between <code>[-1,1]</code> and the standard deviation is 1 and mean 0, you will have to follow <a href="https://stackoverflow.com/a/51755387/2451763?ref=localhost">this accepted StackOverflow answer</a>. Even otherwise with this process, the data has been standardized.</p><h3 id="bucketize">Bucketize</h3><p>Bucketization is done when we have to organize continuous ranges of data into different buckets. <code>Bucketizer</code> allows us to group data based on boundaries, so a list of boundaries has to be provided. I will call it <code>splits</code> with the domain of all buckets when added looks like: <code>{(-∞, -500.0) ⋃ [-500.0, -100.0) ⋃ [-100.0, -10.0) ⋃ [-10.0, 0.0) ⋃ [0.0, 10.0) ⋃ [10.0, 100.0) ⋃ [100.0, 500.0) ⋃ [500.0, ∞)}</code>.</p><p>Then I'll generate 1000 random points that fall in the range of <code>[-10000.0, 10000.0]</code> and save it in a <code>DataFrame</code> with column name as <code>features</code>. This is done using the below code:</p><pre><code class="language-scala">val splits = Array(Float.NegativeInfinity, -500.0, -100.0, -10.0, 0.0, 10.0, 100.0, 500.0, Float.PositiveInfinity)

val bucketData = (for (i &lt;- 0 to 10000) yield math.random * 10000.0 * (if (math.random &lt; 0.5) -1 else 1))
val bucketDf = bucketData.toDF("features")</code></pre><p>Now, our <code>Bucketizer</code> needs three inputs: the splits, input column name, and output column name. Then I'll <code>transform</code> that data which would then give me the element and which bucket it belongs to:</p><figure class="kg-card kg-code-card"><pre><code class="language-scala">val bucketizer = new Bucketizer()
    .setSplits(splits)
    .setInputCol("features")
    .setOutputCol("bfeatures")

val bucketedDf = bucketizer.transform(bucketDf)</code></pre><figcaption>Bucketizing Numeric Data in Spark</figcaption></figure><p>Notice that I didn't have to do a <code>fit</code> operation before doing a <code>transform</code>. This is because Bucketizing is fairly simple and you only need to find which bucket a number belongs to. Thus, there are no operations like scaling which happened in the other 2 sections, and hence you don't need to <code>fit</code> your data. Now if we have a look at the created <code>DataFrame</code>:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="http://localhost:2368/content/images/2020/05/image-9.png" class="kg-image" alt loading="lazy"><figcaption>Bucketized DataFrame</figcaption></figure><p>Now you might also want to know how many numbers are there in a particular bucket. So, I will do a <code>groupBy</code> on <code>bFeatures</code> column and retrieve the count of occurrences. The following code does that and displays my generated data:</p><figure class="kg-card kg-image-card"><img src="http://localhost:2368/content/images/2020/05/image-10.png" class="kg-image" alt loading="lazy"></figure><p>Fairly easy, isn't it?</p><h2 id="text">Text</h2><p>There are two ways in which you can preprocess text-based data in Spark:</p><ul><li>Tokenize</li><li>TF-IDF</li></ul><p>To illustrate both of them, I will be using <code>sentencesDf</code> created using this code:</p><figure class="kg-card kg-code-card"><pre><code class="language-scala">val sentencesDf = Seq(
    (1, "This is an introduction to Spark ML"),
    (2, "MLLib includes libraries for classification and regression"),
    (3, "It also contains supporting tools for pipelines")
).toDF("id", "sentence")</code></pre><figcaption>Creating a <code>DataFrame</code> of sentences</figcaption></figure><h3 id="tokenize">Tokenize</h3><p>In tokenization, you map your string containing a sentence into a set of tokens, or words. As an Example, the sentence <em>"This is an introduction to Spark ML"</em> can be mapped into a list of 7 words - <code>{This, is, an, introduction, to, Spark, ML}</code>.</p><p>We will first import <code>Tokenizer</code> from the <code>org.apache.spark.ml.feature</code> package. Now an instance of this will need two parameters - input column and output column. Our input will be <code>sentence</code> and the output will be <code>words</code>, because that is what the <code>Tokenizer</code> will produce. Then we will apply <code>transform</code> on the sentences above.</p><p>Now, just like bucketing, we are not <code>fit</code>ting any data here. <code>Tokenizer</code> already knows its job - Split strings into the separate words. The above process is illustrated in the code below:</p><pre><code class="language-scala">val sentenceToken = new Tokenizer()
    .setInputCol("sentence")
    .setOutputCol("words")

val sentenceTokenizedDf = sentenceToken.transform(sentencesDf)</code></pre><p>Now, if we have a look at our data:</p><figure class="kg-card kg-image-card"><img src="http://localhost:2368/content/images/2020/05/image-11.png" class="kg-image" alt loading="lazy"></figure><p>The <code>words</code> column contains lists of words that have been broken up in the ways you would expect a regular expression pattern matching to break up a sentence into words - based on white space, punctuation, etc. </p><p>Easy, isn't it?</p><h3 id="term-frequency-inverse-document-frequency-tf-idf-">Term Frequency-Inverse Document Frequency (TF-IDF)</h3><p>Here we map text from a single, typically long string, to a vector, indicating the frequency of each word in a text relative to a group of texts such as a corpus. This transformation is widely used in text classification. </p><figure class="kg-card kg-image-card"><img src="http://localhost:2368/content/images/2020/05/image-12.png" class="kg-image" alt loading="lazy"></figure><p>TF-IDF captures the intuition that infrequently used words are more useful for distinguishing categories of text than frequently used words. Considering the above figure as an example, <em>Normalizing </em>appears only once, <em>to</em> appears twice and so on. Like this, we go through all the documents in our corpus, which is nothing but a collection of documents. Then we count up how often a term appears across all of the documents. In this example <em>normalizing </em>is a very rare word. Whereas other words like <em>maps, data</em> and <em>to</em> show up more frequently. We use these two sets of counts and feed those two into the term frequency-inverse document frequency calculation. And that gives us our TF-IDF measures.</p><p>I will use the same <code>sentenceTokenizedDf</code> created above for this exercise as well. Just like other processes mentioned above, we will need to import a few things from <code>org.apache.spark.ml.feature</code> package - <code>HashingTF</code> (for hashing Term Frequency), <code>IDF</code> (for Inverse Document Frequency), <code>Tokenizer</code>.</p><p>First, I will create a <code>HashingTF</code> instance - which takes an input column (<code>words</code>), an output column (<code>rawFeatures</code>)  and the number of features to keep track of (<code>20</code>) as the parameters. Now we apply our <code>transform</code>ation on this and get a new <code>DataFrame</code>:</p><pre><code class="language-scala">val hashingTF = new HashingTF()
    .setInputCol("words")
    .setOutputCol("rawFeatures")
    .setNumFeatures(20)

val sentenceHashingFunctionTermFrequencyDf = hashingTF.transform(sentenceTokenizedDf)
sentenceHashingFunctionTermFrequencyDf.show()</code></pre><p>Now if we have a look at our data, it has added an extra column which is of <code>Vector</code> type. It has mapped each word to an index, so for example, <em>this</em> maps to 1, <em>is </em>maps to 4, <em>an </em>-&gt; 5, and so on. </p><figure class="kg-card kg-image-card"><img src="http://localhost:2368/content/images/2020/05/image-13.png" class="kg-image" alt loading="lazy"></figure><p>Now we're going to scale the <code>rawFeatures</code> vector values and we're going to scale them based on how often the words appear in the entire collection of sentences. To do this we're going to create an <code>IDF</code> instance. Again, we have to specify an input column (<code>rawFeatures</code>) and an output column (<code>idfFeatures</code>) as parameters.</p><p>Let's use the term frequency data we just calculated to <code>fit</code> the inverse document frequency model. And to do that I'm going to create an <code>idfModel</code>, and we're going to call the <code>idf</code> object I just created, and I'm going to fit it using our term frequency data. Then we apply the IDF <code>transform</code>ation to create a new <code>DataFrame</code> that has both the term frequency and the inverse document frequency transformations applied.</p><pre><code class="language-scala">val idf = new IDF()
    .setInputCol("rawFeatures")
    .setOutputCol("idfFeatures")

val idfModel = idf.fit(sentenceHashingFunctionTermFrequencyDf)
val tfIdfDf = idfModel.transform(sentenceHashingFunctionTermFrequencyDf)</code></pre><p>Now if we have a look at our data (I'm selecting only the <code>rawFeatures</code> and <code>idfFeatures</code> columns to fit in the screen):</p><figure class="kg-card kg-image-card"><img src="http://localhost:2368/content/images/2020/05/image-14.png" class="kg-image" alt loading="lazy"></figure><p>Now we have a new column which contains the inverse document frequency features. These are measures of each word relative to how frequently they occur in the entire corpus. In our case our corpus is just three sentences.</p><h2 id="conclusion">CONCLUSION</h2><p>Preprocessing is indeed a tough challenge where you will have to know what kinds of data you might get and what kinds of processing you want to apply on your data. If not done properly, your machine learning models might not be of much use.</p>
    </section>


</article>
</main>




            <aside class="read-more-wrap outer">
                <div class="read-more inner">
                        
<article class="post-card post featured">

    <a class="post-card-image-link" href="/podman-best-docker-alternative/">

        <img class="post-card-image"
            srcset="/content/images/size/w300/2022/03/podman-vs-docker.jpg 300w,
                    /content/images/size/w600/2022/03/podman-vs-docker.jpg 600w,
                    /content/images/size/w1000/2022/03/podman-vs-docker.jpg 1000w,
                    /content/images/size/w2000/2022/03/podman-vs-docker.jpg 2000w"
            sizes="(max-width: 1000px) 400px, 800px"
            src="/content/images/size/w600/2022/03/podman-vs-docker.jpg"
            alt="Podman with Desktop Companion: The best alternative to Docker Desktop"
            loading="lazy"
        />


    </a>

    <div class="post-card-content">

        <a class="post-card-content-link" href="/podman-best-docker-alternative/">
            <header class="post-card-header">
                <div class="post-card-tags">
                        <span class="post-card-featured"><svg width="16" height="17" viewBox="0 0 16 17" fill="none" xmlns="http://www.w3.org/2000/svg">
    <path d="M4.49365 4.58752C3.53115 6.03752 2.74365 7.70002 2.74365 9.25002C2.74365 10.6424 3.29678 11.9778 4.28134 12.9623C5.26591 13.9469 6.60127 14.5 7.99365 14.5C9.38604 14.5 10.7214 13.9469 11.706 12.9623C12.6905 11.9778 13.2437 10.6424 13.2437 9.25002C13.2437 6.00002 10.9937 3.50002 9.16865 1.68127L6.99365 6.25002L4.49365 4.58752Z" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"></path>
</svg> Featured</span>
                </div>
                <h2 class="post-card-title">
                    Podman with Desktop Companion: The best alternative to Docker Desktop
                </h2>
            </header>
                <div class="post-card-excerpt">Docker Desktop is no longer free for the enterprise. Podman helps you fill the void left by Docker. Know more about it and the installation process in my latest blog post.</div>
        </a>

        <footer class="post-card-meta">
            <time class="post-card-meta-date" datetime="2022-04-03">Apr 3, 2022</time>
                <span class="post-card-meta-length">5 min read</span>
        </footer>

    </div>

</article>
                        
<article class="post-card post featured">

    <a class="post-card-image-link" href="/factors-to-look-dealing-crypto/">

        <img class="post-card-image"
            srcset="https://images.unsplash.com/photo-1621504450181-5d356f61d307?crop&#x3D;entropy&amp;cs&#x3D;tinysrgb&amp;fit&#x3D;max&amp;fm&#x3D;jpg&amp;ixid&#x3D;MnwxMTc3M3wwfDF8c2VhcmNofDN8fGNyeXB0b3xlbnwwfHx8fDE2MjgxMDYyOTU&amp;ixlib&#x3D;rb-1.2.1&amp;q&#x3D;80&amp;w&#x3D;300 300w,
                    https://images.unsplash.com/photo-1621504450181-5d356f61d307?crop&#x3D;entropy&amp;cs&#x3D;tinysrgb&amp;fit&#x3D;max&amp;fm&#x3D;jpg&amp;ixid&#x3D;MnwxMTc3M3wwfDF8c2VhcmNofDN8fGNyeXB0b3xlbnwwfHx8fDE2MjgxMDYyOTU&amp;ixlib&#x3D;rb-1.2.1&amp;q&#x3D;80&amp;w&#x3D;600 600w,
                    https://images.unsplash.com/photo-1621504450181-5d356f61d307?crop&#x3D;entropy&amp;cs&#x3D;tinysrgb&amp;fit&#x3D;max&amp;fm&#x3D;jpg&amp;ixid&#x3D;MnwxMTc3M3wwfDF8c2VhcmNofDN8fGNyeXB0b3xlbnwwfHx8fDE2MjgxMDYyOTU&amp;ixlib&#x3D;rb-1.2.1&amp;q&#x3D;80&amp;w&#x3D;1000 1000w,
                    https://images.unsplash.com/photo-1621504450181-5d356f61d307?crop&#x3D;entropy&amp;cs&#x3D;tinysrgb&amp;fit&#x3D;max&amp;fm&#x3D;jpg&amp;ixid&#x3D;MnwxMTc3M3wwfDF8c2VhcmNofDN8fGNyeXB0b3xlbnwwfHx8fDE2MjgxMDYyOTU&amp;ixlib&#x3D;rb-1.2.1&amp;q&#x3D;80&amp;w&#x3D;2000 2000w"
            sizes="(max-width: 1000px) 400px, 800px"
            src="https://images.unsplash.com/photo-1621504450181-5d356f61d307?crop&#x3D;entropy&amp;cs&#x3D;tinysrgb&amp;fit&#x3D;max&amp;fm&#x3D;jpg&amp;ixid&#x3D;MnwxMTc3M3wwfDF8c2VhcmNofDN8fGNyeXB0b3xlbnwwfHx8fDE2MjgxMDYyOTU&amp;ixlib&#x3D;rb-1.2.1&amp;q&#x3D;80&amp;w&#x3D;600"
            alt="Factors to look at while dealing with Cryptocurrencies"
            loading="lazy"
        />


    </a>

    <div class="post-card-content">

        <a class="post-card-content-link" href="/factors-to-look-dealing-crypto/">
            <header class="post-card-header">
                <div class="post-card-tags">
                        <span class="post-card-featured"><svg width="16" height="17" viewBox="0 0 16 17" fill="none" xmlns="http://www.w3.org/2000/svg">
    <path d="M4.49365 4.58752C3.53115 6.03752 2.74365 7.70002 2.74365 9.25002C2.74365 10.6424 3.29678 11.9778 4.28134 12.9623C5.26591 13.9469 6.60127 14.5 7.99365 14.5C9.38604 14.5 10.7214 13.9469 11.706 12.9623C12.6905 11.9778 13.2437 10.6424 13.2437 9.25002C13.2437 6.00002 10.9937 3.50002 9.16865 1.68127L6.99365 6.25002L4.49365 4.58752Z" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"></path>
</svg> Featured</span>
                </div>
                <h2 class="post-card-title">
                    Factors to look at while dealing with Cryptocurrencies
                </h2>
            </header>
                <div class="post-card-excerpt">Ever wondered how to research crypto tokens before you buy them, but have no idea where to start? You've ended your search at the right place.</div>
        </a>

        <footer class="post-card-meta">
            <time class="post-card-meta-date" datetime="2021-08-06">Aug 6, 2021</time>
                <span class="post-card-meta-length">9 min read</span>
        </footer>

    </div>

</article>
                        
<article class="post-card post featured">

    <a class="post-card-image-link" href="/health-targeted-digital-advertising-today/">

        <img class="post-card-image"
            srcset="/content/images/size/w300/2021/01/targeted-nontargeted-houston-marketing-display-advertising1.jpg 300w,
                    /content/images/size/w600/2021/01/targeted-nontargeted-houston-marketing-display-advertising1.jpg 600w,
                    /content/images/size/w1000/2021/01/targeted-nontargeted-houston-marketing-display-advertising1.jpg 1000w,
                    /content/images/size/w2000/2021/01/targeted-nontargeted-houston-marketing-display-advertising1.jpg 2000w"
            sizes="(max-width: 1000px) 400px, 800px"
            src="/content/images/size/w600/2021/01/targeted-nontargeted-houston-marketing-display-advertising1.jpg"
            alt="Is the state of targeted digital advertising broken?"
            loading="lazy"
        />


    </a>

    <div class="post-card-content">

        <a class="post-card-content-link" href="/health-targeted-digital-advertising-today/">
            <header class="post-card-header">
                <div class="post-card-tags">
                        <span class="post-card-featured"><svg width="16" height="17" viewBox="0 0 16 17" fill="none" xmlns="http://www.w3.org/2000/svg">
    <path d="M4.49365 4.58752C3.53115 6.03752 2.74365 7.70002 2.74365 9.25002C2.74365 10.6424 3.29678 11.9778 4.28134 12.9623C5.26591 13.9469 6.60127 14.5 7.99365 14.5C9.38604 14.5 10.7214 13.9469 11.706 12.9623C12.6905 11.9778 13.2437 10.6424 13.2437 9.25002C13.2437 6.00002 10.9937 3.50002 9.16865 1.68127L6.99365 6.25002L4.49365 4.58752Z" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"></path>
</svg> Featured</span>
                </div>
                <h2 class="post-card-title">
                    Is the state of targeted digital advertising broken?
                </h2>
            </header>
                <div class="post-card-excerpt">Advertisements are everywhere - newspapers, TV channels, billboards, and digital. But is the current state of digital advertising good for the end user? Read my opinion to find out more.</div>
        </a>

        <footer class="post-card-meta">
            <time class="post-card-meta-date" datetime="2021-01-23">Jan 23, 2021</time>
                <span class="post-card-meta-length">11 min read</span>
        </footer>

    </div>

</article>
                </div>
            </aside>



    </div>

    <footer class="site-footer outer">
        <div class="inner">
            <section class="copyright"><a href="http://localhost:2368">Sparker0i&#x27;s Blog</a> &copy; 2024</section>
            <nav class="site-footer-nav">
                
            </nav>
            <div class="gh-powered-by"><a href="https://ghost.org/" target="_blank" rel="noopener">Powered by Ghost</a></div>
        </div>
    </footer>

</div>


<script
    src="https://code.jquery.com/jquery-3.5.1.min.js"
    integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0="
    crossorigin="anonymous">
</script>
<script src="/assets/built/casper.js?v=fe10ead0d3"></script>
<script>
$(document).ready(function () {
    // Mobile Menu Trigger
    $('.gh-burger').click(function () {
        $('body').toggleClass('gh-head-open');
    });
    // FitVids - Makes video embeds responsive
    $(".gh-content").fitVids();
});
</script>

<script>
  var followSocialMedia = {
    youtube: ["https://www.youtube.com/c/sparker0i", "Sparker0i"],
    instagram: ["https://instagram.com/sparker0i", "Sparker0i"],
    github: ["https://github.com/sparker0i", "Sparker0i"],
    linkedin: ["https://www.linkedin.com/in/sparker0i", "Sparker0i"]
  };
</script>

<script>
  var youTube = {
    name: "Sparker0i",
    channelId: "UC720N9wQAevIjCD50x4RliQ",
  };
</script>

<script>
  var disqusShortName = "sparker0is-blog";
</script>

<script>
	var searchSettings = {
        key: "12584d6e462ba2c007d10f133c",
        url: "https://blog.sparker0i.me",
        options: {
          keys: ["title"],
          limit: 10,
        },
        api: {
          resource: "posts",
          parameters: {
            limit: "all",
            fields: ["title", "slug"],
            filter: "",
            include: "",
            order: "",
            formats: "",
          },
        },
    };
</script>

</body>
</html>
