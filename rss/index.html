<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:media="http://search.yahoo.com/mrss/"><channel><title><![CDATA[Sparker0i's Blog]]></title><description><![CDATA[Code | Tech | Cricket]]></description><link>https://blog.sparker0i.me/</link><image><url>https://blog.sparker0i.me/favicon.png</url><title>Sparker0i&apos;s Blog</title><link>https://blog.sparker0i.me/</link></image><generator>Ghost 5.54</generator><lastBuildDate>Tue, 09 Apr 2024 15:52:57 GMT</lastBuildDate><atom:link href="https://blog.sparker0i.me/rss/" rel="self" type="application/rss+xml"/><ttl>60</ttl><item><title><![CDATA[How to use Devcontainers to create apps with x86 architecture on an Apple Silicon Mac]]></title><description><![CDATA[Certain apps and libraries never want to update with time, thus you end up with emulation. Here's how you would develop apps using incompatible libraries on your Apple Silicon Mac.]]></description><link>https://blog.sparker0i.me/running-vs-code-devcontainers-with-x86-runtime-apple-silicon/</link><guid isPermaLink="false">6614c29bb8fb0c1426a84179</guid><category><![CDATA[Virtualization]]></category><category><![CDATA[Colima]]></category><category><![CDATA[Docker]]></category><category><![CDATA[Devcontainer]]></category><category><![CDATA[Rosetta]]></category><category><![CDATA[VS Code]]></category><category><![CDATA[VSCode]]></category><category><![CDATA[Visual Studio Code]]></category><dc:creator><![CDATA[Aaditya Menon]]></dc:creator><pubDate>Tue, 09 Apr 2024 15:51:58 GMT</pubDate><media:content url="https://blog.sparker0i.me/content/images/2024/04/Rosetta2.jpg" medium="image"/><content:encoded><![CDATA[<img src="https://blog.sparker0i.me/content/images/2024/04/Rosetta2.jpg" alt="How to use Devcontainers to create apps with x86 architecture on an Apple Silicon Mac"><p>Apps are a significant part of our lives today. There are various apps you might be using today. On a smartphone, you would be using WhatsApp, Snapchat, Instagram, YouTube and various other apps. On a PC/laptop, you would be using a browser, game launchers to start your favorite games and IDEs to develop applications. </p>
<p>Many websites you know and love are apps themselves. As an example, the Facebook website is written using the React framework and packaged as a Web application to run in a browser. YouTube and various other websites by Google are written in the Angular framework and packaged as web apps too.</p>
<p>To develop any kind of major applications, you would need a PC or a laptop and an IDE installed. There are various kinds of IDEs available based on the programming language and the kind of application you are developing. You would also need various libraries to create your application - lest write the code yourself. Which leads to the problem I&apos;ll be tackling in today&apos;s post.</p>
<h2 id="background">Background</h2>
<p>PCs and laptops sold today run on x86 architecture CPUs made by Intel and AMD. However in recent times, we have started to see a lot of laptops being sold with the CPUs using ARM architecture, which until recently was found only in mobile phones. Not only are these CPUs way more battery efficient, they also allow to wake a laptop from sleep a lot quicker than x86 based laptops. Most notable ARM based laptops are manufactured by Apple, which use the Apple Silicon chips - M1, M2, M3 etc.</p>
<p>Like I&apos;ve explained before when you are looking to build an application, you&apos;d need various libraries to write code. Most libraries in the various programming languages are universal, ie. they are compatible to run on the architecture of your machine&apos;s CPU. However, there are some libraries which do not run (yet) on an ARM machine.</p>
<p>The most notable culprit for this is the <a href="https://www.npmjs.com/package/ibm_db?ref=localhost">ibm_db</a> library on Node. If you try to install that package on your Apple Silicon mac, you will see this error:</p>
<figure class="kg-card kg-image-card"><img src="https://blog.sparker0i.me/content/images/2024/04/Screenshot-2024-04-09-at-1.47.30-PM.jpg" class="kg-image" alt="How to use Devcontainers to create apps with x86 architecture on an Apple Silicon Mac" loading="lazy" width="2000" height="212" srcset="https://blog.sparker0i.me/content/images/size/w600/2024/04/Screenshot-2024-04-09-at-1.47.30-PM.jpg 600w, https://blog.sparker0i.me/content/images/size/w1000/2024/04/Screenshot-2024-04-09-at-1.47.30-PM.jpg 1000w, https://blog.sparker0i.me/content/images/size/w1600/2024/04/Screenshot-2024-04-09-at-1.47.30-PM.jpg 1600w, https://blog.sparker0i.me/content/images/2024/04/Screenshot-2024-04-09-at-1.47.30-PM.jpg 2004w" sizes="(min-width: 720px) 720px"><figcaption><span>Error installing ibm_db directly on the Apple Silicon MacBook</span></figcaption></figure>
<p>Yup, it suggests to install the x64 version of NodeJS and then use the package. I had so many other NodeJS applications on my machine without ibm_db which were working pretty well, so I did not want to install an inefficient version of NodeJS for my machine. But I also had to work on this important project on the M1 mac for my org. I was in a dilemma. Enter <strong>devcontainers</strong>:</p>
<h2 id="devcontainers">Devcontainers</h2>
<p>From its website, A devcontainer allows you to use a container as a full-featured development environment. It can be used to run an application, to separate tools, libraries, or runtimes needed for working with a codebase, and to aid in continuous integration and testing.</p>
<p>This is very similar to how Python&apos;s venv (Virtual Environments) work. Usually, all Python developers need that to do any basic development. But one key difference with devcontainers is that it opens your project folder inside a Docker container, and then any packages you install in the devcontainer remains inside that and does not cross over to your host machine.</p>
<p>While the devcontainer spec is Open source and available independently, Visual Studio Code provides an easy way (UI) of doing stuff with it. Using devcontainers, I&apos;ll be trying to run my NodeJS app with the ibm_db dependency on my MacBook with Apple Silicon.</p>
<h2 id="create-the-virtual-machine">Create the Virtual Machine</h2>
<p>Docker - or for that matter any of the open source containerization software - cannot run as is on a machine without a Linux Kernel. You&apos;ll need a Linux virtual machine that acts as the place where all your containers will be run. The simplest solution to do this is to create a VM using Colima.</p>
<p>Here is the command to create a machine using Colima: </p>
<pre><code>colima start --cpu 2 --memory 4 --disk 50 --arch aarch64 --vm-type=vz --mount-type=virtiofs --vz-rosetta --very-verbose</code></pre>
<ul><li>The CPU, Memory and Disk parameters should be very obvious here.</li><li><code>--very-verbose</code> is to see more detailed logging while the VM starts. It is useful to debug if anything is going wrong or not.  </li><li><code>--arch aarch64</code> tells the Lima CLI to create the VM with the ARM64 architecture. You cannot directly create an x86 machine on an ARM MacBook just like that. The next two options will help enable what I want to do.</li><li><code>--vm-type=vz</code> will use the new MacOS <a href="https://developer.apple.com/documentation/virtualization?language=objc&amp;ref=localhost">Virtualization API</a> to create the VMs.</li><li><code>--vz-rosetta</code> will use Rosetta translation layer when interacting with the VM.</li><li><code>--mount-type=virtiofs</code> creates a VM with the virtiofs volume driver. This allows you to share files from your host machine inside the Container.</li></ul>
<p>Using the VZ APIs along with the virtiofs mount enables better performance running the VM.</p>
<p>Create base image for the Container</p>
<p>Devcontainer runs your code inside a Docker container. The basic principle of running a Docker Container requires you to have a base image on top of which any operations can be performed.</p>
<p>In this case I want to develop a NodeJS app which utilizes the ibm_db library. So I will need a base image with NodeJS installed. Thankfully, Microsoft provides base Docker images that work well with devcontainers inside VS Code. I&apos;ll be using a base image which comes with NodeJS 20 installed.</p>
<p>To create the base image I will use in my application, I will need to SSH into the Virtual Machine using the command and then create the base image from there. If you try to create the base image from your host terminal, the image gets created with the ARM architecture, which is not helpful for us as we want the image with an x86 architecture.</p>
<p>This is the <code>Dockerfile</code> I will be using:</p>
<figure class="kg-card kg-code-card"><pre><code class="language-Dockerfile">FROM --platform=linux/amd64 mcr.microsoft.com/devcontainers/typescript-node:20-bookworm
RUN uname -a</code></pre><figcaption><p><span>Dockerfile contents</span></p></figcaption></figure>
<p>To create the image, run the following commands:</p>
<ol><li><code>colima ssh</code></li><li><code>export DOCKER_DEFAULT_PLATFORM=linux/amd64</code></li><li><code>docker build --no-cache --platform linux/amd64 --progress plain -t node20-amd64-localhost:latest .</code></li><li><code>exit</code></li></ol>
<p>Once you build this image, you should see the output like below for the <code>uname</code> command. If you don&apos;t see the <code>x86_64</code> like I&apos;ve highlighted, then you might have not followed the guide properly:</p>
<figure class="kg-card kg-image-card"><img src="https://blog.sparker0i.me/content/images/2024/04/Screenshot-2024-04-09-at-4.06.30-PM.jpg" class="kg-image" alt="How to use Devcontainers to create apps with x86 architecture on an Apple Silicon Mac" loading="lazy" width="1544" height="116" srcset="https://blog.sparker0i.me/content/images/size/w600/2024/04/Screenshot-2024-04-09-at-4.06.30-PM.jpg 600w, https://blog.sparker0i.me/content/images/size/w1000/2024/04/Screenshot-2024-04-09-at-4.06.30-PM.jpg 1000w, https://blog.sparker0i.me/content/images/2024/04/Screenshot-2024-04-09-at-4.06.30-PM.jpg 1544w" sizes="(min-width: 720px) 720px"></figure>
<h3 id="run-your-project-inside-a-devcontainer">Run your project inside a Devcontainer</h3>
<p>I won&apos;t be using any sample project for this article, as you may use any of your x86 based projects you wish to emulate inside a Devcontainer. To do that, you will need to create a folder called <code>.devcontainer</code>, inside which you need to have two files: <code>devcontainer.json</code> and <code>Dockerfile</code>.</p>
<p>The contents of the <code>Dockerfile</code> is a single line which uses the base image that we had built in one of the previous sections:</p>
<pre><code class="language-Dockerfile">FROM --platform=linux/amd64 node20-amd64-localhost:latest</code></pre>
<p>The <code>devcontainer.json</code> would contain the following contents (Please do not copy the comments as is, it is only meant to explain what each line does:</p>
<pre><code class="language-json">{
  &quot;name&quot;: &quot;NodeJS with Typescript installed to build angular apps using x86-only libraries&quot;,
  &quot;dockerfile&quot;: &quot;Dockerfile&quot;,
  &quot;runArgs&quot;: [&quot;-v&quot;, &quot;${localWorkspaceFolder}:/workspace:cached&quot;],
  &quot;customizations&quot;: {
    &quot;vscode&quot;: {
      &quot;extensions&quot;: [
        &quot;ms-azuretools.vscode-docker&quot;
      ]
    }
  },
  &quot;remoteUser&quot;: &quot;root&quot;,
  &quot;forwardPorts&quot;: [3000]
}</code></pre>
<p>In line 4, <code>${localWorkspaceFolder}</code> refers to your project&apos;s location on the host machine, <code>/workspace</code> is where your project files be mounted inside the devcontainer and the <code>:cached</code> option is used to improve performance in Docker when mounting volumes.</p>
<p>I want to use the user <code>root</code> inside my devcontainer so that I don&apos;t need to do a <code>sudo</code> everytime to install an npm package inside the container. I&apos;m forwarding the port 3000 from my container to my host machine as my NodeJS express app uses that port to listen onto requests.</p>
<p>Once you are done with these two files, open your project inside VSCode, then install the <a href="https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers&amp;ref=localhost">Devcontainers</a> extension, Reload the window, press Cmd+Shift+P and then type <code>Reopen in Container</code> and click on that option. This will build your Container image, mount your project and make it available inside <code>/workspace</code> in the container and then you should be able to emulate projects using x86 libraries inside the devcontainer on your machine.</p>
<p>To test that it works, I will try to install the <code>ibm_db</code> package from within the container, and here&apos;s how that went:</p>
<figure class="kg-card kg-image-card"><img src="https://blog.sparker0i.me/content/images/2024/04/Screenshot-2024-04-09-at-8.12.39-PM.jpg" class="kg-image" alt="How to use Devcontainers to create apps with x86 architecture on an Apple Silicon Mac" loading="lazy" width="1212" height="452" srcset="https://blog.sparker0i.me/content/images/size/w600/2024/04/Screenshot-2024-04-09-at-8.12.39-PM.jpg 600w, https://blog.sparker0i.me/content/images/size/w1000/2024/04/Screenshot-2024-04-09-at-8.12.39-PM.jpg 1000w, https://blog.sparker0i.me/content/images/2024/04/Screenshot-2024-04-09-at-8.12.39-PM.jpg 1212w" sizes="(min-width: 720px) 720px"><figcaption><span>That went nicely.</span></figcaption></figure>
<p>Now I need to run my app and see whether it is able to connect to my Database using this library or not: </p>
<figure class="kg-card kg-image-card"><img src="https://blog.sparker0i.me/content/images/2024/04/Screenshot-2024-04-09-at-8.26.14-PM.jpg" class="kg-image" alt="How to use Devcontainers to create apps with x86 architecture on an Apple Silicon Mac" loading="lazy" width="1320" height="228" srcset="https://blog.sparker0i.me/content/images/size/w600/2024/04/Screenshot-2024-04-09-at-8.26.14-PM.jpg 600w, https://blog.sparker0i.me/content/images/size/w1000/2024/04/Screenshot-2024-04-09-at-8.26.14-PM.jpg 1000w, https://blog.sparker0i.me/content/images/2024/04/Screenshot-2024-04-09-at-8.26.14-PM.jpg 1320w" sizes="(min-width: 720px) 720px"><figcaption><span>Yup it did connect well.</span></figcaption></figure>
<h3 id="why-am-i-using-colima">Why am I using Colima</h3>
<p>Colima has support for emulating x86 based VMs using the Rosetta 2 translation layer on Apple Silicon Macs. This is important as we needed ibm_db to work. I also think that building x86 images on ARM platforms will become common, as soon as ARM based laptops from Apple and others start becoming mainstream.</p>
<p>Moreover, I&apos;ve not yet found another easier way to create and run a Docker machine using CLI commands. Of course there&apos;s Docker Desktop which gives a nice GUI, but its <a href="https://www.docker.com/blog/updating-product-subscriptions/?ref=localhost">license change</a> in 2022 wrecked havoc on many companies. Our org had to ban the installs of Docker Desktop completely. I have had to migrate to Podman <a href="https://blog.sparker0i.me/podman-best-docker-alternative/?ref=localhost">in the past</a> due to this. While it was fun, it didn&apos;t help me solve my problem. Which brings us to:</p>
<h3 id="why-not-podman">Why not Podman?</h3>
<p>As of writing this blog in early April 2024, Podman does not yet support x86_64 emulation using the Rosetta translation layer. There is a Pull Request open that tackles the issue, however it&apos;s not merged yet. So we&apos;d have to wait and see how that pans out, and I shall give devcontainers a try with Podman once the issues with Rosetta are sorted.</p>
<h3 id="why-not-qemu">Why not QEMU?</h3>
<p>I did try using QEMU based emulation by typing <code>colima start --arch x86_64 -p qemu</code> but, While that worked okay, as in it started the container and I was able to run my app, I discovered that for my NodeJS based application it wasn&apos;t really as efficient. Also, my M1 MacBook was heating up like it hadn&apos;t done ever before. What is the point of having an M1 Mac if it&apos;s going to behave the same as the Intel ones. Thus I felt using Rosetta based emulation was better for me.</p>
<h3 id="how-about-windows">How about Windows?</h3>
<p>While ARM based Windows laptops are set to make a debut later in 2024, I don&apos;t believe there will be too much to be done to get it to work. There&apos;s WSL which exists already, and one has to watch out how the x86 emulation plays out on these ARM machines using the Snapdragon X chips. If that ends up like how Rosetta has played out so far, all we&apos;ll need is for WSL and its distros to support doing the same as well. Things are not yet clear on that front, and I will try to update my article as soon as the picture is clear.</p>
<h2 id="conclusion">Conclusion</h2>
<p>Right now I&apos;ve just shown one example where I had to run an NodeJS app with x86 libraries on an M1 based Mac without spinning up a full fledged VM like inside VirtualBox or VMWare. You can also extend this concept to various other languages having x86-only libraries like Python etc.</p>]]></content:encoded></item><item><title><![CDATA[Podman with Desktop Companion: The best alternative to Docker Desktop]]></title><description><![CDATA[Docker Desktop is no longer free for the enterprise. Podman helps you fill the void left by Docker. Know more about it and the installation process in my latest blog post.]]></description><link>https://blog.sparker0i.me/podman-best-docker-alternative/</link><guid isPermaLink="false">623ec4aa2239e2119bff8b34</guid><category><![CDATA[Docker]]></category><category><![CDATA[Podman]]></category><category><![CDATA[Docker Desktop]]></category><dc:creator><![CDATA[Aaditya Menon]]></dc:creator><pubDate>Sun, 03 Apr 2022 11:31:44 GMT</pubDate><media:content url="https://blog.sparker0i.me/content/images/2022/03/podman-vs-docker.jpg" medium="image"/><content:encoded><![CDATA[<img src="https://blog.sparker0i.me/content/images/2022/03/podman-vs-docker.jpg" alt="Podman with Desktop Companion: The best alternative to Docker Desktop"><p>Containers and Docker have always been synonymous to the ears since eternity. For most of us, Docker has been the go-to tool for containerization - a way to pack a software application and all dependencies into an image that can be run anywhere. This allows application to remain separate from the infrastructure so that the app works regardless of where it runs.</p><p>While working on multiple applications in my company, I had been using Docker Desktop as my containerization platform of choice, on my company provided MacBooks to build and debug application images. But we were dealt with a blow a few months ago when it was announced that effective January 31st 2022, Docker Desktop will <a href="https://www.docker.com/blog/updating-product-subscriptions/?ref=localhost">require a license for enterprises</a> with more than 250 employees. Unfortunately, my company IBM fell into Docker&apos;s categorization of an enterprise.</p><p>Naturally, I started looking at all available alternatives, given that Docker desktop annual subscription per user costs $250. Unfortunately there was no clear winner which could replicate everything that Docker Desktop could do - GUI support, Built in Kubernetes, Environments, Integration with IDEs etc. But all I needed was something with which I could build images locally in a secure manner and run containers locally, with an optional support of a GUI.</p><p>Enter:</p><h2 id="podman">Podman</h2><p>Podman is a CLI tool that provides a Docker-compatible API. It is open source and published by Red Hat. The biggest advantage over Docker is that it is a Daemonless container engine which runs the containers in a rootless state by default. This brings in additional security layer, because even if the container engine, runtime or the orchestrator is compromized, the attacker won&apos;t gain any root privileges on the host - which is a flaw in Docker&apos;s architecture. You can read <a href="https://www.imaginarycloud.com/blog/podman-vs-docker?ref=localhost">this article</a> to understand the finer points of difference between Docker and Podman.</p><p>Installation of Podman 4 is fairly simple on MacOS (using brew). Unfortunately for Linux based Operating Systems, only Fedora has an unofficial COPR that allows you to install Podman 4, while for other Operating Systems you have to build from source code in order to install podman 4.</p><p>Once you install the podman binary, all you need to do is execute the below two commands for MacOS:</p><pre><code>podman machine init
podman machine start</code></pre><p>This will start a Fedora CoreOS based VM in the background having podman installed. Once you have started this Podman VM, Almost all Docker CLI commands are compatible with Podman as well - <code>run</code>, <code>exec</code>, <code>push</code> etc. It brings zero impact to the developers that operate on CLI - how you build images and whatever you do with that image using the Docker CLI remains the same. You can also add <code><a href="https://podman.io/whatis.html?ref=localhost">alias docker=podman</a></code> to <code>~/.zshrc</code> (MacOS) or <code>~/.bash_profile</code> (Linux) so that you don&apos;t need to keep reminding yourself to use podman instead of Docker.</p><p>The CLI route is easy for users who have been working with Docker CLI. However what about those users who have been primarily using the Docker Desktop GUI for their workflows? Enter:</p><h2 id="podman-desktop-companion">Podman Desktop Companion</h2><p><a href="https://iongion.github.io/podman-desktop-companion/?ref=localhost">Podman Desktop Companion</a> is a third-party app, which is an almost adequate drop-in replacement for the Docker Desktop GUI. Since this is not an official app, there are a few features this app lacks, most notably the absence of Kubernetes - though this won&apos;t be a big deal for those who only want the containerization features of Podman. Here&apos;s a screenshot of the app running on my MacBook:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://blog.sparker0i.me/content/images/2022/03/Screenshot-2022-03-26-at-10.30.34-PM.png" class="kg-image" alt="Podman with Desktop Companion: The best alternative to Docker Desktop" loading="lazy" width="2000" height="1549" srcset="https://blog.sparker0i.me/content/images/size/w600/2022/03/Screenshot-2022-03-26-at-10.30.34-PM.png 600w, https://blog.sparker0i.me/content/images/size/w1000/2022/03/Screenshot-2022-03-26-at-10.30.34-PM.png 1000w, https://blog.sparker0i.me/content/images/size/w1600/2022/03/Screenshot-2022-03-26-at-10.30.34-PM.png 1600w, https://blog.sparker0i.me/content/images/2022/03/Screenshot-2022-03-26-at-10.30.34-PM.png 2272w" sizes="(min-width: 720px) 720px"><figcaption>Podman Desktop Companion makes you feel right at home with the look and feel of Docker Desktop</figcaption></figure><p>Feels familiar, doesn&apos;t it? Unfortunately, the process to install this app is not as straightforward. For a MacBook, you will need to install <code>lima-vm</code>, an app that launches Linux virtual machines with automatic file sharing and port forwarding - very similar to what WSL2 does, just that it is mostly for MacOS but can also be used on various Linux distros as well. Unfortunately if you want to proceed to the next step, you will have to stop the <code>podman machine</code> you had created earlier.</p><p>Lima offers the ability to create VMs using their sample YAML templates or by supplying user written YAML. When you create a VM with the Podman Template YAML, the VM will be running Podman v3. This lacks some key features from Podman 4 including Volume and Device mounts, as well as a vastly improved Network stack. Thus you will have to use a custom YAML if you want to install Podman 4. </p><p>You need to ensure that the name of this VM is <code>podman</code>. This is necessary for the Desktop Companion (This dependency on a specific VM name is a bad programming practice on the part of the Desktop Companion creator, but since this is a Beta version we can forgive them for now until they release a stable version). You can do this by running <code>limactl start --name=podman /location/of/the/yaml/from/below</code>. To make things easier for you, I&apos;ve written a YAML that worked well for me. You need to save this to a location and use the location of this YAML in the command above.</p><pre><code class="language-yaml">images:
- location: &quot;https://download.fedoraproject.org/pub/fedora/linux/releases/35/Cloud/x86_64/images/Fedora-Cloud-Base-35-1.2.x86_64.qcow2&quot;
  arch: &quot;x86_64&quot;
  digest: &quot;sha256:fe84502779b3477284a8d4c86731f642ca10dd3984d2b5eccdf82630a9ca2de6&quot;
- location: &quot;https://download.fedoraproject.org/pub/fedora/linux/releases/35/Cloud/aarch64/images/Fedora-Cloud-Base-35-1.2.aarch64.qcow2&quot;
  arch: &quot;aarch64&quot;
  digest: &quot;sha256:c71f2e6ce75b516d565e2c297ea9994c69b946cb3eaa0a4bbea400dbd6f59ae6&quot;
cpus: 4
memory: 8 GiB
disk: 50 GiB
mounts:
- location: &quot;~&quot;
- location: &quot;/tmp/lima&quot;
  writable: true
containerd:
  system: false
  user: false
provision:
- mode: system
  script: |
    #!/bin/bash
    dnf copr enable rhcontainerbot/podman4 -y
    dnf update
    dnf install -y podman crun
- mode: user
  script: |
    #!/bin/bash
    set -eux -o pipefail
    systemctl --user enable --now podman.socket
probes:
- script: |
    #!/bin/bash
    set -eux -o pipefail
    if ! timeout 30s bash -c &quot;until command -v podman &gt;/dev/null 2&gt;&amp;1; do sleep 3; done&quot;; then
      echo &gt;&amp;2 &quot;podman is not installed yet&quot;
      exit 1
    fi
  hint: See &quot;/var/log/cloud-init-output.log&quot;. in the guest
portForwards:
- guestSocket: &quot;/run/user/{{.UID}}/podman/podman.sock&quot;
  hostSocket: &quot;{{.Dir}}/sock/podman.sock&quot;
message: |
  To run `podman` on the host (assumes podman-remote is installed), run the following commands:
  ------
  export CONTAINER_HOST=$(limactl list podman --format &apos;unix://{{.Dir}}/sock/podman.sock&apos;)
  podman system connection add lima &quot;unix://{{.Dir}}/sock/podman.sock&quot;
  podman system connection default lima
  podman{{if eq .HostOS &quot;linux&quot;}} --remote{{end}} run quay.io/podman/hello
  ------</code></pre><p>I&apos;m using a Fedora Base VM Image because there is a custom COPR available which can install Podman v4 in the VM image. Then once I&apos;m done installing Podman and crun on the VM, I&apos;m forwarding the VM&apos;s socket to a socket on the host machine. This is needed so for the Desktop companion to establish connection and verify services running on the guest VM.</p><p>Once the VM is installed and started, you also need 4 other commands on the host to notify the podman remote CLI to connect to the guest VM rather than listening on the host. Thus once you are done installing Lima, Podman Remote CLI on Host, Podman on Lima VM and the Desktop Companion, you will feel right at home, without missing Docker Desktop.</p>]]></content:encoded></item><item><title><![CDATA[Factors to look at while dealing with Cryptocurrencies]]></title><description><![CDATA[Ever wondered how to research crypto tokens before you buy them, but have no idea where to start? You've ended your search at the right place.]]></description><link>https://blog.sparker0i.me/factors-to-look-dealing-crypto/</link><guid isPermaLink="false">61f016ac0598f2d200c5e161</guid><category><![CDATA[Cryptocurrency]]></category><category><![CDATA[Bitcoin]]></category><dc:creator><![CDATA[Aaditya Menon]]></dc:creator><pubDate>Fri, 06 Aug 2021 04:20:59 GMT</pubDate><media:content url="https://images.unsplash.com/photo-1621504450181-5d356f61d307?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDN8fGNyeXB0b3xlbnwwfHx8fDE2MjgxMDYyOTU&amp;ixlib=rb-1.2.1&amp;q=80&amp;w=2000" medium="image"/><content:encoded><![CDATA[<img src="https://images.unsplash.com/photo-1621504450181-5d356f61d307?crop=entropy&amp;cs=tinysrgb&amp;fit=max&amp;fm=jpg&amp;ixid=MnwxMTc3M3wwfDF8c2VhcmNofDN8fGNyeXB0b3xlbnwwfHx8fDE2MjgxMDYyOTU&amp;ixlib=rb-1.2.1&amp;q=80&amp;w=2000" alt="Factors to look at while dealing with Cryptocurrencies"><p>Crypto-currencies are a hot subject today. From seeing multiple bull-runs, to governments trying to destroy the movement of sovereign independence, &#xA0;to countries adopting Bitcoin as a legal tender, cryptocurrencies have come a long way. I have been holding crypto-currencies since 2019, but it was only at the start of this year that I actually started trading and buying them.</p><p>Now, if you ask anyone online about which crypto to buy, you will be frequently met with the term #DYOR - Do Your Own Research. Same was with me as well, whenever I&apos;ve asked people on tips, I was told to #DYOR, but there were not many tips online about what to do in DYOR. Like many in the beginning, I&apos;ve had my shares of success and failures in crypto hodling and trading. Here are a few tips I follow when looking to buy tokens.</p><h2 id="initial-research-and-filtering-bad-assets">Initial Research and Filtering Bad Assets</h2><p>Before I hodl or trade any tokens, I make sure to do some initial research with a few pointers. I&apos;d feel great about those tokens that meet this initial criteria, while others become a big no-go for the rest of my life (unless something drastically changes in that particular crypto&apos;s tech, but would be unlikely).</p><p>The first thing I do is visit CoinMarketCap website, search for the particular crypto I&apos;m looking to hodl or invest. Every major crypto asset listed on CoinMarketCap will have its own website, and that is the first thing I always look at. Within the website, the first thing you should read is the <strong>Whitepaper</strong> of the token. When you read through the various pages on a particular crypto&apos;s website, I am able to mostly detect any non-sense or BS. If it uses any terms or protocols that you find it hard to comprehend - like it &quot;<strong>promises</strong>&quot; a huge payout or claims to have some tech that seems to good to be true (like self-mining) - immediately avoid it. </p><p>You should also look for some more things like Roadmap of features for the particular project and the track record on how it has delivered so far on all those features. If a project has consistently failed or lagged behind to deliver new features, you should treat it as another yellow flag. I&apos;d also look at how the crypto&apos;s tech functions and the value a token is trying to create. If there are no unique use-cases or if the crypto does not require/benefit from the use of blockchain fundamentals, I&apos;d give it a red flag and move on.</p><p>Another thing that you should really look at is the Blockchain Explorer for a given crypto. First thing I&apos;d check here is the max supply for the given crypto. Needless to say, stablecoins are a no-go here. If there&apos;s no max supply for a token, it is usually a Red Flag from my side, although there are a very few exceptions for this - like Ethereum. If you still want to go ahead and deal with cryptos without a max supply, you should do it at your own risk and must usually look to own it only for the short-term and not hodl them.</p><p>Next thing you should look at is the distribution of tokens across all wallet holders of the given token. If you see that one wallet holds more than 15-20% of the total current supply, I&apos;d raise a red flag here. The moment a big whale dumps (sells) all those coins, the entire crypto will start reeling. Best example would be Vitalik Buterin <a href="https://www.nzherald.co.nz/business/ethereum-co-founder-vitalik-buterin-destroys-410-trillion-shiba-inu-coins/VCNH2FDFQYRVDD6U3Q6VFYZ2XQ/?ref=localhost">dumping 45% of the entire Shiba Inu coins in supply</a>, causing a massive drop in the price of the Shiba Inu coin. (More surprising in the case of Shiba Inu is that the Whitepaper of the coin still refers Buterin as a visionary - &quot;like its anonymous founder&quot; - even after he dumped the project&apos;s worth)</p><p>Some other pointers that you can consider to research would be browsing the source code of a given asset on GitHub and the amount of activity in its repos in the recent past. As an alternate means, try going through the blogs and subreddit of the given token and look for any threads that look skeptical to you.</p><h2 id="avoid-scams">Avoid Scams</h2><p>When you start off dealing with cryptos, you would be looking at ways to maximize money instantly. While I&apos;d say it is not impossible, but I would advise you to not do that. Lured by the opportunity of making quick money, people might fall into the trap of crypto scams.</p><p>Most of these scams happen on Telegram within groups which usually advertise quick money making schemes in the form of Token Airdrops. They will mostly look to lure you with &quot;verified&quot; token deposit screenshots from customers. These &quot;customers&quot; are either running the scam along with the admins of the group, or are accounts managed by the admins themselves. If you look to participate in such airdrops, you will end up losing all your life savings.</p><p>However, I&apos;m not saying all airdrops are bad. Some of them that have happened - like the Stellar Airdrops in 2017 and 2019. But that was done by a verified entity - the Stellar Foundation. Here we are talking about Telegram groups that are run by Anonymous Users. Such groups are always Bait and Scam, and I would recommend you to avoid them.</p><h2 id="fill-out-an-investment-checklist">Fill out an Investment Checklist</h2><p>Once I&apos;m done filtering out the coins I invest in, I look to create an investment checklist where I have noted certain questions that I ask before I invest in a coin. Here are the details of the questions:</p><ul><li><strong>What is the problem that a crypto is trying to solve? </strong>Like I mentioned in the previous section, if a token does not serve a purpose or does not solve a problem - not just with blockchains or the financial sector, but any sector, say Education or Supply Chain - I believe there is no reason for that coin to exist. Such coins are only hype driven and only good for short-term trading. I will call such hype-driven coins as <strong>Shitcoins</strong> from now. Hodling them for a long term will leave you reeling.</li><li><strong>What is the Dev Team like? What is their track record? How are they funded, organized?</strong> I will look for is the history of some of the Lead members on a project and see what they have worked on previously - via information they post online (say on LinkedIn). If they are anonymous members (exception: Satoshi Nakamoto, the creator of Bitcoin), I&apos;ll automatically raise a Red flag on that crypto and move on. Then, I will also look at how they are funded. Whether they have partnerships with Enterprises or they have solved a critical problem for any pillar of the society - say government or law - or any other major piece of news regarding any advancement that was made possible because of a crypto.</li><li><strong>Who is their competition and how big is the market they&apos;re targeting?</strong> What is the roadmap they created? Like I&apos;ve mentioned in the previous section, you should always have a look at the roadmap for a given token. No roadmap -&gt; No progress -&gt; Red Flag. Also you need to look at the other coins being offered in the market and how do they compete with the token you&apos;re looking to buy. Because healthy competition always leads to success, unless you&apos;re looking for a shitcoin, or the competitors of a given token are all shitcoins.</li><li><strong>Is there a staking mechanism or is it transactional?</strong> There are cryptos which rely on various types of consensus mechanisms - Proof of Work (PoW), Proof of Stake (PoS), Proof of Time and Space etc.</li><li><strong>How does the token/coin actually derive value for the holder?</strong></li><li><strong>What are the weaknesses or problems with this crypto?</strong> Of course there can never be some shortcoming associated with a given crypto. If there are none, it sounds too good to be true. (Even for Bitcoin, there&apos;s only one disadvantage that I can think about - Electricity consumption. But that too is definitely lesser than electricity used for mining actual Gold, or by the entire banking system around the world)</li></ul><h2 id="create-a-valuation-framework-for-a-token">Create a Valuation Framework for a token</h2><p>With the boom that crypto trading has seen since November 2020, you must always be sure of what coins you want to invest in. I&apos;ve been a part of various crypto groups on multiple sites like Facebook and Reddit. In it, I&apos;ve seen a lot of people complain about the losses they&apos;ve had to made while dealing with crypto, because they either bought high and then the market went bearish, or they invested in the pump-and-dump shitcoins. </p><p>To avoid becoming one among such people, you need to have a certain valuation model for a given crypto. Even if you create a basic one, you&apos;ll go miles ahead of your peers/other people. Here are some simple things you can consider doing:</p><ul><li>Look at the total market cap of the coin and compare it with the size of the market it is trying to address. Market cap here includes not only the circulating supply, but also the max supply possible for the crypto.</li><li>Check for the total number of users of a given token. I believe that the value of a given crypto would be proportional to the number of users using the crypto. The best known crypto out there is Bitcoin and there are almost 100 Million people already using it. Compounded with its scarcity, its scale and community is why Bitcoin carries a huge value.</li></ul><p>These were 2 very basic checks that I do for any crypto. I might add some other checks in the evaluation framework as well. Once you have a model setup, you can then evaluate one crypto against another, and also see why you would want to own this coin rather than do short-term trading. Doing this will lead you to think long term about dealing with a given crypto and think more about the &quot;value&quot; it provides. </p><p>Once you do this, you will have more mental peace and have good confidence for your decision making. You would also not panic when there are any short-term price dips.</p><h2 id="portfolio-allocation">Portfolio Allocation</h2><p>This is one of the crucial aspects of your journey with crypto. You should think on how much fiat you can use to buy crypto, as well as how do you want to allocate your crypto portfolio between &quot;safe&quot; and &quot;rigged&quot; cryptos. (planned pump-and-dump, shitcoins etc.). If you&apos;re just starting out, I would recommend you to not go with shitcoins to begin with. I have seen a few of my friends who started their crypto journeys because of celebrities who rig coins like Elon Musk and co. The peak of this came during Elon&apos;s SNL appearance. Despite my advice against it, most of my friends bought Dogecoin only because of Elon&apos;s appearance and thinking it&apos;s price will keep going up if they bought then. Little did they know that it was the starting point for dumping Dogecoin, whose effects can still be felt today. Many of my friends felt cheated, and some of them have left the crypto space despite my assurances that not all cryptos are bad.</p><p>So if you&apos;re just starting out, I&apos;d recommend having 80-90% of your portfolio filled with safe coins. Once you become more informed and confident, you can start dealing with Shitcoins and vary your portfolio allocation. You should also think in terms of crypto categories as well as the percentage of your portfolio in each of these segments. For starters, I categorise crypto into following categories: (You may disagree with how I&apos;m grouping the cryptos, but we can agree to disagree)</p><ul><li>Core Holdings: Bitcoin (BTC), Litecoin (LTC)</li><li>Smart Contracts: Ethereum (ETH), Cardano (ADA), Polkadot (DOT), ALGO, Solana etc.</li><li>Crypto with Privacy?: Monero (XMR), ZCash</li><li>Intermediary with fiat dealings/Bank Settlement: Stellar (XLM), Ripple (XRP)</li><li>Enterprise Solutions: VeChain etc.</li><li>Promising: NANO, IOTA, RVN, Algo etc.</li><li>Coins without intrinsic value (Shitcoin): DOGE + the army of dog coins (Shiba, Kishu) etc.</li></ul><p>You should also have a fair idea about the market conditions as there is a lot of uncertainty today, mostly due to policies+governments and others due to rigging shitcoins. Due to this uncertainty, you should best stick to the core holdings and then pick up coins in segments you are knowledgeable enough about. Example, if you don&apos;t know about Smart Contracts, how can you be sure that ALGO is a game changer?</p><p>This is where portfolio diversification and allocation comes into place. You should look to diversify, but in the ever-changing world of crypto you shouldn&apos;t look to diversify way too much. It would be difficult for you to keep up with all the changes that have occurred across cryptos. I wouldn&apos;t recommend you to have more than 10-12 cryptos. If you have more, I would recommend you to consolidate to the few segments you are well knowledgeable about.</p><h2 id="learn-everyday">Learn Everyday</h2><p>If you aren&apos;t doing this already, read a bit daily on cryptocurrencies. There are decent YouTubers that talk about the market movements in the crypto space. You should also learn more about the underlying principles in the crypto space. More specifically, if you are not aware of basics like Proof-of-Work (PoW), Proof-of-Space (PoS), learn about it first.</p><p>If you invest in stocks, one factor you would look for is what are the core offerings of a given company and how it performs against other peers. If you don&apos;t know about how the underlying technology of a crypto works, learn about it as well. That will increase your belief if you want to buy that crypto. If you don&apos;t care about the underlying technology or find reading about it very tedious, I believe you shouldn&apos;t be in this crypto space at all and shouldn&apos;t look to invest here, otherwise you will feel betrayed after your coins&apos; value gets dumped.</p>]]></content:encoded></item><item><title><![CDATA[Is the state of targeted digital advertising broken?]]></title><description><![CDATA[Advertisements are everywhere - newspapers, TV channels, billboards, and digital. But is the current state of digital advertising good for the end user? Read my opinion to find out more.]]></description><link>https://blog.sparker0i.me/health-targeted-digital-advertising-today/</link><guid isPermaLink="false">61f016ac0598f2d200c5e160</guid><category><![CDATA[Opinion]]></category><category><![CDATA[Ads]]></category><category><![CDATA[Digital Ads]]></category><category><![CDATA[Internet Ads]]></category><category><![CDATA[Advertising]]></category><dc:creator><![CDATA[Aaditya Menon]]></dc:creator><pubDate>Sat, 23 Jan 2021 03:33:00 GMT</pubDate><media:content url="https://blog.sparker0i.me/content/images/2021/01/targeted-nontargeted-houston-marketing-display-advertising1.jpg" medium="image"/><content:encoded><![CDATA[<img src="https://blog.sparker0i.me/content/images/2021/01/targeted-nontargeted-houston-marketing-display-advertising1.jpg" alt="Is the state of targeted digital advertising broken?"><p>Advertisements are everywhere - You see ads on your newspaper, your family sees ads on TV channels, there are ads on billboards seen by people in and around an area, and you too see ads on almost any website you visit.</p><p>But, internet advertising industry is broken. It&apos;s failing the users and the advertisers themselves. Internet advertising majorly uses targeted/personalized ads to go after you and cater products you would most likely want. This ends up collecting as much data about you as it can.</p><p>In this blog post, I will argue why the current model of internet advertising is bad, and how can you save yourself from being a victim of personalized ad targeting.</p><h2 id="a-regular-user-s-perspective-on-advertising">A Regular User&apos;s Perspective on Advertising</h2><p>If you ask me what are the problems with the current model of advertising online, here are a few things I might suggest:</p><ul><li>Ads violate privacy: There&apos;s no secret to this, internet advertising scoops up as much information about you from the services you visit and use.</li><li>Ads are disruptive: As an example, sometimes you click on a blank area in a website, you then see a new pop-up window with an ad, despite your browser blocking pop-ups</li><li>Ads are annoying: Sometimes you visit an interesting website, and the very next moment when you open a social media website, you start seeing ads about this almost everytime.</li><li>Malicious Ads: A vast majority of the people of my generation shouldn&apos;t be worried about Malicious ads, as our generation is aware about websites and malware, and we&apos;d only visit website after due diligence. But our previous generations are not so well-informed about websites, they would click on any website and that site may end up spreading malware.</li></ul><p>What I&apos;m worried about is the first point - Digital Advertising violating our privacy. While our previous generations are not much aware about their Data Privacy, our current generation does worry about it.</p><p>While we sit here, worrying about our privacy, IMF has &quot;suggested&quot; that a person&apos;s <a href="https://gizmodo.com/your-credit-score-should-be-based-on-your-web-history-1845912592?ref=localhost">credit history should be based on their Web History</a>. That&apos;s how ridiculous things may get in the future if things are not corrected today. A 2018 study by researchers at Data and Society <a href="https://datasociety.net/wp-content/uploads/2018/10/DS_Digital_Influence_Machine.pdf?ref=localhost">concluded </a>that &#x201C;today&#x2019;s digital advertising infrastructure creates disturbing new opportunities for political manipulation and other forms of antidemocratic strategic communication.&#x201D;</p><p>Even without concerns of privacy violations, there have been many efforts to try and fix some of the issues:</p><ul><li><a href="https://www.betterads.org/?ref=localhost">Coalition for Better Ads</a></li><li><a href="https://acceptableads.com/?ref=localhost">Acceptable Ads</a></li><li>Brave</li></ul><p>These only fix some of the problems with Advertising in general, not the entire problem. </p><h2 id="attention-economy">Attention Economy</h2><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://blog.sparker0i.me/content/images/2021/01/digital-billboards.jpg" class="kg-image" alt="Is the state of targeted digital advertising broken?" loading="lazy"><figcaption>Multiple advertisements craving for your attention</figcaption></figure><p><a href="https://medium.com/rsa-journal/democracy-distracted-cf3272ceb3c4?ref=localhost">Herbert Simon argued in the 70s </a>that when information becomes abundant, attention becomes the scarce resource. In today&apos;s modern age, we&#x2019;re living through the pendulum swing of that reversal&#x2014;yet we consistently overlook its implications. </p><p>When you visit any website or browse through any app, you take some time out of your life - or your <strong>attention</strong> to visit that resource. You are also using some of your attention to read this post (for which I&apos;m eternally grateful). Now, you normally wouldn&apos;t take out some of this attention to visit a website that you haven&apos;t clicked by yourself. Thus, such websites/apps allow to advertise within itself (mostly as a small banner) via publishers, so that you don&apos;t have to take out extra time off your life just to visit the standalone ad. Then, such publishers use your &quot;actions&quot;, and use such individualized data about you - collected via algorithms - and sell slots to advertisers to display ads.</p><p>Wikipedia has a perfect definition for this - <a href="https://en.wikipedia.org/wiki/Attention_economy?ref=localhost">Attention Economy</a>.</p><p>Who wouldn&apos;t love to have an individualized feed of news. Sadly, individualization that happens today is neither for our own benefit nor for getting a peace of mind. Rather such a system only invites more outrage from us - users.</p><p>Now, an incentive driven advertising system is good for the publishers, who earn good money; and advertisers, who get to sell lot more of their products. But this is bad for the user. There are literally billions of dollars being spent to figure out how to persuade you to look at a certain thing over a competitor&apos;s; to care about one thing over another. This is the way most of the information in this world is being monetized today.</p><p>We can all see the effects of Attention Economy - Social Media websites like Twitter and Facebook never want people of different groups to stop fighting - be it over religion, sports or their political bias. This is because these networks are optimized in a way to promote fighting, or spread fake news/propaganda very quickly. You may argue that companies are investing to stop fake news, but I believe that it is still relatively minor as they don&apos;t really want their revenue streams to go down.</p><h2 id="my-take-on-digital-ads">My take on digital ads</h2><p>I don&#x2019;t want any relevant ads, or any ad-driven products, ever. Other people might feel differently, but collecting the data is only half the problem. That data can be abused, even if only client-side - for example targeting could exploit you and persuade to buy stuff you don&#x2019;t need. The whole point of ads is to change your behavior, if only slightly, with advertisers literally bidding for your attention, and then playing mind tricks, which work because of how our critical thinking often fails us a majority of the times.</p><p>I believe great products have often been successful via word of mouth referrals. A great example would be <strong>Cred</strong>. Only a few credit card users in India were aware of the Cred before September 2020. That month, a delayed IPL 2020 started, and Cred ads - which a majority of the Indians never understood - was everywhere on TV, getting people&apos;s interest in knowing about the product, and eventually end up using it. There are tactics that don&#x2019;t involve tricking people into changing their behavior. Conventional advertising is better than the targeted ones, because it&#x2019;s not individualized. At least everybody else is seeing the same shit that you&#x2019;re seeing.</p><p>That brings us to the most important question - What should companies do? Honestly speaking, I don&apos;t think that&apos;s our problem. I don&apos;t think any company should be entitled to our data or our attention. When was the last time you ever saw an ad that made you feel this is the real deal. I haven&apos;t, have you?</p><p>You know how the unethical behavior of companies is always excused via the requirement of company stakeholders to make money? Well, this goes both ways. We, the consumers, aren&#x2019;t running any charity. I strongly feel it isn&#x2019;t our job to save such dying business models.</p><h3 id="first-party-ads">First Party Ads</h3><p>One solution to the problem could be the content providers (websites/apps) serving ads based on content that is being viewed right now. This might ensure there is no need to keep user&apos;s profile and history. Your data will no longer be shared to third-parties and hence the quality of ads shown will get better - which also means no scam ads and no malvertising.</p><p>This too has a problem. Even if there is a whitelist for &#x201C;acceptable ads&#x201D;, the problem is those ads are still downloaded Javascript code, with no way to review what they do. Further, third-party requests could get disguised as first-party ones. In such cases, you have to trust the publishers, which I&apos;d usually find a hard time to do.</p><h3 id="then-wouldn-t-ad-blocking-affect-the-internet">Then, Wouldn&apos;t Ad-Blocking affect the Internet?</h3><p>Given a choice between receiving ads, and no ads, sane people would obviously choose no ads. Nobody likes ads. If its forced, we have to gulp it down our throat, because either there&apos;s no clear alternative (Facebook), or the price of no-ads option is way too high (Youtube, Spotify, Apple Music, Hotstar et al.).</p><p>Almost no browser ships with an aggressive ad-blocker enabled by default. This is because somewhere or the other, their business model is also dependent on ads, even if they don&apos;t serve ads directly. It will only make the end user&apos;s experience much better. Many browsers that claim to do aggressive ad-blocking like Microsoft Edge, Brave, Samsung, Vivaldi et al. earn a lot of money from advertising.</p><p>Now, wouldn&apos;t ad-blocking affect the Internet&apos;s health? Yes, it will. Negatively, on the advertisers and publishers&apos; wallets; Positively on the rest of the internet, because it can only get better from there on.</p><h2 id="solutions">Solutions</h2><h2 id="pay-for-the-things-you-use">Pay for the things you use</h2><p>A wise man once said,</p><blockquote>&quot;If you are not paying for a product, <strong>you are the product</strong>&quot;.</blockquote><p>If I really enjoy something, I don&apos;t mind paying for it. For example, I&apos;ve been using <strong>Hey email</strong> since July. It did not catch my attention because of all the amazing features it brings, but rather because of its infamous <a href="https://hey.com/apple/?ref=localhost">feud with Apple</a>. In short, Hey is a radical re-thinking of what an Email client should be. As one Twitter user puts it:</p><figure class="kg-card kg-embed-card"><blockquote class="twitter-tweet"><p lang="en" dir="ltr">In positive news, <a href="https://t.co/lwKSOQNcFe?ref=localhost">https://t.co/lwKSOQNcFe</a> seems to have finally solved email (!!!). Been using it several weeks and no longer dealing with spam, long lists of &#x201C;unread&#x201D; messages, or sorting out annoying but important docs. The relief is so real &#x1F64C;&#x1F3FC;</p>&#x2014; Dr. Darya Rose &#x1F1FA;&#x1F1F8; (@summertomato) <a href="https://twitter.com/summertomato/status/1270816132102930433?ref_src=twsrc%5Etfw&amp;ref=localhost">June 10, 2020</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</figure><p>I&apos;ve been using it so far without any major problems. All advertising mails are now screened out, so that they never take my attention again. I&apos;ve changed emails for all my online accounts to reflect to my hey.com email address. Now there are certain services like stock and mutual fund brokers, whose email change process has to be offline. Once all of my services are moved over to Hey, I&apos;m deleting all my non Hey mail accounts (except primary Google Play Store and Microsoft Store accounts, because all my app/in-app purchase history is sitting in those 2 accounts).</p><p>Similarly, I&apos;m also using <strong>Spotify Premium</strong> because I don&apos;t want myself to be exposed to their ads right after one song.</p><p>I also pay for <strong>Xbox Game Pass for PC</strong>. It has more than 100 AAA PC games that I can easily play on my Lenovo Legion Y740 laptop. There are no ads in the games that are played via XGP. I regularly play Age of Empires 2: DE, Flight Simulator, Doom Eternal, Moto GP20 and The Outer Worlds. Buying all those games individually would&apos;ve cost me a fortune, but at Rs 489/month (577 if you include taxes) it is a killer deal, specially for gamers like me. I am also using <strong>Kindle Unlimited</strong> for Reading as many Books as I can.</p><p>With all the information I&apos;m unearthing every single day about the shady practices of all developers, I&apos;m removing all ads-enabled apps from my mobiles. Moreover, if I find that any app is sharing data that is unwanted, I promptly uninstall that app. If at all I really need to use the app&apos;s services which provide ads, I would find a browser-based alternative for it - Eg. Youtube, Twitter, Facebook, IG, Amazon + Entertainment apps.</p><h2 id="block-all-ads">Block All Ads</h2><p>Even if you may think that some ads are fine, but do you know whether those are targeted specifically to you or not? Even if there&apos;s a whitelist of &quot;acceptable ads&quot;, it&apos;s still Javascript code downloaded from the internet, which I have no idea about its functionality. It&apos;s still intrusive, and drives you to buy things you don&apos;t really want.</p><p>I really think it is wrong that Publishers think they can run code on our devices. If publishers think otherwise, they can feel free to block me from using their services. If I see any website that serves me ads (and gets caught by my ad blockers), I&apos;d really start looking for alternatives.</p><h3 id="blocking-for-all-devices">Blocking for all devices</h3><p>DNS blocking has worked well so far. That is until apps/sites starting using DNS-over-HTTPS over their own servers. Still there are techniques to circumvent this too.</p><p><strong><u>PiHole</u></strong>: I had a Raspberry Pi 3B sitting idle at home for the last 1 year or so. When I got to know that I could run a service like Pihole on it, I wasted no time to set it up. Ever since Christmas eve, I&apos;ve been using Pihole to block ads for all the devices my family uses. And the stats are great too:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://blog.sparker0i.me/content/images/2021/01/image.png" class="kg-image" alt="Is the state of targeted digital advertising broken?" loading="lazy"><figcaption>Pihole stats since December 23, 2020</figcaption></figure><p>Though the only downside to this is that I&apos;ve had to change my router to use static IP address assignment. There was some initial hassle in setting up my router and the devices, but once that was done, ads were a thing of the past (except Youtube ads, for which there is no working solution in 2021. Screw you, Google).</p><p><strong><u>Hosted services</u></strong>: 3 months before trying out Pihole, I tried using NextDNS. While the results were incredible, I felt that giving away Rs. 159 every month (187 including taxes) was a bit too much for me. I&apos;ve also heard about Adguard DNS, but I haven&apos;t used it so far.</p><h2 id="individual-devices">Individual Devices</h2><h3 id="desktop-laptop-android-devices">Desktop/Laptop + Android devices</h3><p>Even before I&apos;ve been using PiHole, I had experimented with ad blocking via browser extensions. The best combination I&apos;ve found is Firefox + uBlock Origin + Privacy Badger. It simply works.</p><p>uBlock Origin is by far the best ad blocker I&apos;ve ever used. Ever since installing it, I&apos;ve noticed far better loading speeds, because there are lesser resources to be loaded. It also protects against CNAME cloaking, where 3rd party requests masquerade as 1st party ones and you might not even notice it.</p><p>Sadly, this won&apos;t work with Google Chrome and other Chromium forked browsers in the near future. This is because Manifest v3 for Chrome extensions will be deployed in Google Chrome sometime in January (with the release of v88). Simply put, Manifest is something any extension developer will have to follow while developing an extension. This particular Manifest v3 changes the way ad-blocking extensions function. This negatively impacts ad-blockers and uBlock is one of them.</p><p>My take on this will be to ditch Chrome/Chromium forked browsers ASAP. Switch to Firefox while you still can, because Mozilla&apos;s financials aren&apos;t great, and the browser could go kaput anytime.</p><p>The same combination also works well for Android as well, just that you will have to update the Firefox browser to whichever latest version is available.</p><h3 id="what-about-brave-browser">What about Brave Browser?</h3><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://blog.sparker0i.me/content/images/2021/01/ezgif.com-gif-maker.jpg" class="kg-image" alt="Is the state of targeted digital advertising broken?" loading="lazy"><figcaption>Brave browser displaying ads from Brave Network via OS notifications</figcaption></figure><p>Brave is yet another Chromium fork which focuses on user privacy by blocking trackers, scripts and ads by default. So inside some pages where ads don&apos;t load properly, you might be forced to turn down Brave Shields, which will then open up the website to show you ads.</p><p>By the time you&apos;ve reached here, you must&apos;ve realised that internet is up and running, all thanks to advertisements. The content you might be seeing for free is able to remain free because of advertising. This is profitable for both advertisers and publishers, but not the users.</p><p>Brave attempts to change the advertising model by flipping the pyramid upside down. What I mean is, users get paid for getting advertisements on their devices. It allows users to opt-in to its own Brave rewards system (in other words, another advertising system). Brave serves ads after blocking ads from third party providers on websites, and then start showing ads from their own advertising platform via OS level notifications.</p><p>Once you&apos;ve opted-in, Brave will display you &apos;privacy-respecting&apos; ads (it&apos;s still an advertisement btw &#x1F602;) &#xA0;and if you receive an ad, you will be rewarded with a BAT cryptocurrency (BAT = Basic Attention Token), which you can use to support Brave verified creators (You can&apos;t withdraw those tokens to your crypto wallet yet, that&apos;s coming soon though). </p><p>Brave intends to serve ads as per user interests, in an anonymised way. This could be either via client-side profiling, or simply piggybacking what a user is currently viewing. While rewards and ads maybe opt-in for users, they are not for publishers, leaving us in a very awkward position. If a publisher wants a cut of the rewards, they need to be a part of the Brave Partner program.</p><p>With this behaviour, I feel that Brave is yet another middleman between the user and the advertiser. Other companies also have the same model of running their business. You are trusting an advertising company (Brave) - just like Google - to preserve your privacy. What an irony.</p><h3 id="conclusion">Conclusion</h3><p>I want the targeted advertisement system to die. I will go to all lengths to block all ads. I feel companies are not entitled to my data or my time (&apos;attention&apos;).</p><p><u>Read More:</u></p><ul><li><a href="https://practicaltypography.com/the-cowardice-of-brave.html?ref=localhost">The cowardice of Brave</a></li><li><a href="https://openlibrary.org/works/OL19744680W/Ten_arguments_for_deleting_your_social_media_accounts_right_now?ref=localhost">Ten arguments for deleting your Social Media account right now</a></li><li><a href="https://news.ycombinator.com/item?id=18734999&amp;ref=localhost">Brave taking Cryptocurrency donations &quot;for me&quot; without my consent</a></li></ul>]]></content:encoded></item><item><title><![CDATA[How to run Spark 3.0 applications on your GPU]]></title><description><![CDATA[Wanted to run your Spark job on a GPU but didn't know how? Well, your search ends here :)]]></description><link>https://blog.sparker0i.me/run-spark-3-applications-on-gpu/</link><guid isPermaLink="false">61f016ac0598f2d200c5e15e</guid><category><![CDATA[Spark]]></category><category><![CDATA[Scala]]></category><category><![CDATA[Graphics]]></category><category><![CDATA[Nvidia]]></category><category><![CDATA[GPU]]></category><dc:creator><![CDATA[Aaditya Menon]]></dc:creator><pubDate>Sat, 19 Sep 2020 03:44:00 GMT</pubDate><media:content url="https://blog.sparker0i.me/content/images/2020/09/untitled-3-.png" medium="image"/><content:encoded><![CDATA[<img src="https://blog.sparker0i.me/content/images/2020/09/untitled-3-.png" alt="How to run Spark 3.0 applications on your GPU"><p>In one of my previous blog posts, I&apos;d mentioned that Spark 3.0 is coming with Native GPU support. A few days after that, Spark 3.0 released on 18th June 2020. While it did release, there were no mentions of how to run your Spark 3.0 code on a GPU anywhere on the internet. <strong>It changes now.</strong></p><p>In this post, you&apos;ll see the prerequisites for running Spark on GPU on a local machine, as well as all installation instructions.</p><h3 id="prerequisites">Prerequisites</h3><p>To run Spark applications on your GPU, it is recommended that you have an <strong>Nvidia GPU </strong>of<strong> Pascal Architecture</strong> or better. This means that you will need an <strong>Nvidia Geforce GTX 1050 or better</strong>. Other requirements are the same as Apache Spark requirements.</p><p><em>(PS. I don&apos;t have an AMD GPU, so can&apos;t really test and confirm whether this will work with it or not, but chances are very slim as you need a tool called <code>nvidia-smi</code>, which works only with Nvidia GPUs)</em></p><p>You will also need to install <a href="https://spark.apache.org/downloads.html?ref=localhost">Apache Spark 3.0</a>, <a href="https://developer.nvidia.com/cuda-downloads?ref=localhost">Nvidia CUDA</a> on your machine.</p><p>Other than these, you will also need 2 JARs: <a href="https://mvnrepository.com/artifact/com.nvidia/rapids-4-spark_2.12?ref=localhost">Rapids Accelerator</a> and <a href="https://repo1.maven.org/maven2/ai/rapids/cudf/0.15/?ref=localhost">NVIDIA CUDF</a> (for CUDA 11).</p><p>You will also need a Linux system to run your jobs. This won&apos;t work on Windows as CUDF isn&apos;t supported on that platform. However, the CUDF team says they will support CUDA Running on WSL 2.0. To get CUDA Running with WSL, you&apos;ll need to be a part of the Windows Insider Program.</p><p>You will also need a GPU Discovery script which tells the program the addresses of GPUs available on your system. Fortunately, the Spark repo has a <a href="https://github.com/apache/spark/blob/master/examples/src/main/scripts/getGpusResources.sh?ref=localhost">GPU discovery script</a> handy which can be readily used.</p><h3 id="running">Running</h3><p>For Spark 3.0 to recognize that you will be running your jobs on a GPU, you need to pass a few parameters as Spark confs:</p><ul><li><code>spark.rapids.sql.enabled</code> as <code>true</code></li><li><code>spark.plugins</code> as <code>com.nvidia.spark.SQLPlugin</code></li><li><code>spark.driver.resource.gpu.discoveryScript</code> as &lt;The location where you have downloaded the GPU discovery script from above&gt;</li></ul><p>You can either run this with <code>spark-shell</code> or you can create your own JAR and run it using <code>spark-submit</code> and then pass these configurations.</p><h3 id="performance">Performance</h3><p>In order to illustrate the performance difference between running your Spark program on a CPU vs GPU, I will be using a very simple program which is very much self explanatory:</p><figure class="kg-card kg-code-card"><pre><code>val values: List[Int] = List(100, 500, 1000, 5000, 10000, 50000, 100000, 500000, 1000000, 5000000, 10000000, 50000000, 100000000, 500000000, 1000000000)

for (upperBound &lt;- values) {
    val df = sc.makeRDD(1 to upperBound).toDF(&quot;a&quot;)
    val df2 = sc.makeRDD(1 to upperBound).toDF(&quot;b&quot;)
    println(df.join(df2, $&quot;a&quot; === $&quot;b&quot; / 2).count())
}</code></pre><figcaption>Spark program for testing performance: CPU vs GPU</figcaption></figure><p>Further, in order to level the playing field between the 2 runs, I&apos;m setting certain common configs:</p><ul><li><code>spark.locality.wait</code> = <code>0s</code></li><li><code>spark.driver.memory</code> = <code>10G</code></li><li><code>spark.sql.files.maxPartitionBytes</code> = <code>512 * 1024 * 1024</code></li><li><code>spark.sql.shuffle.partitions</code> = <code>10</code></li></ul><p>Here are the specs of the laptop which I used to perform this test:</p><ul><li>6-core Intel Core i7-8750H</li><li>16GB DDR4 RAM, 256GB NVME SSD</li><li>8GB Nvidia Geforce RTX 2080 Graphics Card</li></ul><p>Here are two plots showing the <code>upperBound</code> against time taken:</p><!--kg-card-begin: html--><!DOCTYPE html>
<html>
	<head>
		<title>chart created with amCharts | amCharts</title>
		<meta name="description" content="chart created using amCharts live editor">
		
		<!-- amCharts javascript sources -->
		<script type="text/javascript" src="https://www.amcharts.com/lib/3/amcharts.js"></script>
		<script type="text/javascript" src="https://www.amcharts.com/lib/3/serial.js"></script>
		

		<!-- amCharts javascript code -->
		<script type="text/javascript">
			AmCharts.makeChart("chartdiv",
				{
					"type": "serial",
					"categoryField": "category",
					"startDuration": 1,
					"categoryAxis": {
						"gridPosition": "start",
						"title": "Record Count"
					},
					"trendLines": [],
					"graphs": [
						{
							"balloonText": "Time taken to process [[category]] records in [[title]]:[[value]]",
							"bullet": "round",
							"id": "AmGraph-1",
							"title": "CPU",
							"valueField": "CPU"
						},
						{
							"balloonText": "Time taken to process [[category]] records in [[title]]:[[value]]",
							"bullet": "square",
							"id": "AmGraph-2",
							"title": "GPU",
							"valueField": "GPU"
						}
					],
					"guides": [],
					"valueAxes": [
						{
							"axisFrequency": -2,
							"id": "ValueAxis-1",
							"maximum": 2,
							"minimum": 0,
							"title": "Time taken (s)"
						}
					],
					"allLabels": [],
					"balloon": {},
					"legend": {
						"enabled": true,
						"useGraphSettings": true
					},
					"titles": [
						{
							"id": "Title-1",
							"size": 15,
							"text": "Spark Performance: CPU vs GPU"
						}
					],
					"dataProvider": [
						{
							"category": "100",
							"CPU": "0.3",
							"GPU": "1.3"
						},
						{
							"category": "500",
							"CPU": "0.054",
							"GPU": "0.219"
						},
						{
							"category": "1000",
							"CPU": "0.052",
							"GPU": "0.114"
						},
						{
							"category": "5000",
							"CPU": "0.055",
							"GPU": "0.11"
						},
						{
							"category": "10000",
							"CPU": "0.056",
							"GPU": "0.127"
						},
						{
							"category": "50000",
							"CPU": "0.2",
							"GPU": "0.1"
						},
						{
							"category": "100000",
							"CPU": "0.073",
							"GPU": "0.137"
						},
						{
							"category": "500000",
							"CPU": "0.1",
							"GPU": "0.136"
						},
						{
							"category": "1000000",
							"CPU": "0.2",
							"GPU": "0.159"
						},
						{
							"category": "5000000",
							"CPU": "1",
							"GPU": "0.292"
						},
						{
							"category": "10000000",
							"CPU": "2",
							"GPU": "0.505"
						}
					]
				}
			);
		</script>
	</head>
	<body>
		<div id="chartdiv" style="width: 100%; height: 400px; background-color: #FFFFFF;"></div>
	</body>
</html><!--kg-card-end: html--><p></p><!--kg-card-begin: html--><!DOCTYPE html>
<html>
	<head>
		<title>chart created with amCharts | amCharts</title>
		<meta name="description" content="chart created using amCharts live editor">
		
		<!-- amCharts javascript sources -->
		<script type="text/javascript" src="https://www.amcharts.com/lib/3/amcharts.js"></script>
		<script type="text/javascript" src="https://www.amcharts.com/lib/3/serial.js"></script>
		

		<!-- amCharts javascript code -->
		<script type="text/javascript">
			AmCharts.makeChart("chartdiv1",
				{
					"type": "serial",
					"categoryField": "category",
					"startDuration": 1,
					"categoryAxis": {
						"gridPosition": "start",
						"title": "Record Count"
					},
					"trendLines": [],
					"graphs": [
						{
							"balloonText": "Time taken to process [[category]] records in [[title]]:[[value]]",
							"bullet": "round",
							"id": "AmGraph-1",
							"title": "CPU",
							"valueField": "CPU"
						},
						{
							"balloonText": "Time taken to process [[category]] records in [[title]]:[[value]]",
							"bullet": "square",
							"id": "AmGraph-2",
							"title": "GPU",
							"valueField": "GPU"
						}
					],
					"guides": [],
					"valueAxes": [
						{
							"axisFrequency": -2,
							"id": "ValueAxis-1",
							"maximum": 900,
							"minimum": 0,
							"title": "Time taken (s)"
						}
					],
					"allLabels": [],
					"balloon": {},
					"legend": {
						"enabled": true,
						"useGraphSettings": true
					},
					"titles": [
						{
							"id": "Title-1",
							"size": 15,
							"text": "Spark Performance: CPU vs GPU"
						}
					],
					"dataProvider": [
						{
							"category": "100000000",
							"CPU": "34",
							"GPU": "3.004"
						},
						{
							"category": "500000000",
							"CPU": "318",
							"GPU": "167.004"
						},
						{
							"category": "1000000000",
							"CPU": "840",
							"GPU": "324.004"
						}
					]
				}
			);
		</script>
	</head>
	<body>
		<div id="chartdiv1" style="width: 100%; height: 400px; background-color: #FFFFFF;"></div>
	</body>
</html><!--kg-card-end: html--><p>As you can see from the graphs above, for very less records - with sizes within a few Megabytes - it is faster on the CPU than on the GPU because of the less time taken to propagate the results.</p><p>But things change for the better, when a high volume of records have to start processing. For very high records, you can see a difference of almost 3x. </p><p>Moreover, for 1000000000 records (the last one), my Spark program crashed when run against the CPU. So the 13 minutes that you see above was until when it was successfully running.</p><h3 id="conclusion">Conclusion</h3><p>To confirm whether your program is running against the GPU or not, you can go to the SQL tab, select your job, and then you will see something like <code>GpuRowToColumnar</code>, indicating that the job is running against the GPU.</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://blog.sparker0i.me/content/images/2020/09/Screenshot-from-2020-09-13-23-16-37.png" class="kg-image" alt="How to run Spark 3.0 applications on your GPU" loading="lazy"><figcaption>Spark running on GPU</figcaption></figure><p>So if you&apos;ve got heavy workloads, try and offload them to the GPU as much as you can :)</p>]]></content:encoded></item><item><title><![CDATA[Cool Spark ML - Part 2: Preprocessing of Data]]></title><description><![CDATA[Data preprocessing is the process of detecting and correcting inaccurate records from a dataset. Its importance is more paramount when it comes to big data. Learn how you can preprocess data in Spark ML.]]></description><link>https://blog.sparker0i.me/spark-ml-data-preprocessing/</link><guid isPermaLink="false">61f016ac0598f2d200c5e15d</guid><category><![CDATA[Spark]]></category><category><![CDATA[Spark ML]]></category><dc:creator><![CDATA[Aaditya Menon]]></dc:creator><pubDate>Sat, 06 Jun 2020 08:02:00 GMT</pubDate><media:content url="https://blog.sparker0i.me/content/images/2020/05/mllib.jpg" medium="image"/><content:encoded><![CDATA[<img src="https://blog.sparker0i.me/content/images/2020/05/mllib.jpg" alt="Cool Spark ML - Part 2: Preprocessing of Data"><p><em>Note: This is the second article of the series: Cool Spark ML. The other parts can be found below:</em></p><ul><li><a href="https://blog.sparker0i.me/spark-machine-learning-knn/">Part 1: K Nearest Neighbours</a></li><li>Part 2: Preprocessing of Data (current)</li></ul><p>People who have been performing Machine Learning for quite a long time know that Data Preprocessing is a key step before running any algorithms on the data. In a majority of datasets, you might always find null, or incomplete values. The data would also be inconsistent across columns, which directly affects algorithms using distance measures.</p><p>This is where Data Preprocessing comes in. It is a crucial step which involves cleaning and organizing the data to make it suitable for building models. In other words, if you don&apos;t perform Preprocessing, your models may not be accurate.</p><p>While there are quite a lot of articles online about Data Preprocessing in Python, there aren&apos;t a lot of them in Spark, or even Scala. In this post, I will be dealing with the ways you can perform Data Preprocessing in Spark on Scala.</p><p><em>PS. You might be asking why I&apos;m dealing with this now when I have actually written KNN in Spark before. The truth is, KNN isn&apos;t officially supported inside Spark ML module. What I wrote in the previous article was a top-to-bottom version of KNN performed using Spark. You can also say that I&apos;m doing a complete reset of this series &#x1F605;</em></p><p>Just like always, the codes for all posts in this series will be available on <a href="https://github.com/Sparker0i/Cool-Spark-ML?ref=localhost">my GitHub repo</a>.</p><h2 id="types-of-preprocessing-in-spark">Types of Preprocessing in Spark</h2><p>There are two types of preprocessing:</p><ul><li>Numeric Data</li><li>Text Data</li></ul><h2 id="numeric-data">Numeric Data</h2><figure class="kg-card kg-image-card"><img src="https://blog.sparker0i.me/content/images/2020/05/1_GAcEj37smCOCZMrqp-rjjA.png" class="kg-image" alt="Cool Spark ML - Part 2: Preprocessing of Data" loading="lazy"></figure><p>There are three ways you can preprocess numeric data in Spark:</p><ul><li>Normalize</li><li>Standardize</li><li>Bucketize</li></ul><p>To illustrate Normalize and Standardize, I&apos;ll be using some Scala magic which will generate my points as a Vector. Each vector represents a point in a 3-Dimensional Space.</p><figure class="kg-card kg-code-card"><pre><code class="language-scala">val points = for (i &lt;- 1 to 1000) yield (i, Vectors.dense(
    Array(
        (math.random * (10 - 1)) * i + 1.0,
        (math.random * (10000 - 1000)) + 1000.0,
        math.random * i
    )
))

val featuresDf = points.toDF(&quot;id&quot;, &quot;features&quot;)</code></pre><figcaption>Creating a Random Set of Points in a 3D space</figcaption></figure><p>Doing the above results in the following <code>DataFrame</code>:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://blog.sparker0i.me/content/images/2020/05/image-2.png" class="kg-image" alt="Cool Spark ML - Part 2: Preprocessing of Data" loading="lazy"><figcaption>Each element inside Features column represents a point in a 3-D space.</figcaption></figure><h3 id="normalize">Normalize</h3><p>Normalization is the process of mapping numeric data from their original range into a range of 0 to 1. The lowest value of the original range gets value of 0, and the highest gets the value 1. All the other values in the original range will fall between these two. </p><p>This is important because there may be multiple attributes with different ranges. <em>E.g. Salary values may range between 3 and 8+ digit numbers, years in company will be between 1- and 2-digit numbers. </em>The reason we want to normalize those attributes in a <code>[0,1]</code> range is so that when algorithms that use distance as a measure, they don&apos;t weigh some attributes like salary more heavily than others.</p><p>The formula to convert values in an un-normalized column to a normalized form is given by:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://blog.sparker0i.me/content/images/2020/05/image-5.png" class="kg-image" alt="Cool Spark ML - Part 2: Preprocessing of Data" loading="lazy"><figcaption>Normalization Formula</figcaption></figure><p>Where:</p><ul><li><code>x</code> is the value inside a column to be normalized, </li><li><code>x(new)</code> is the normalized value,</li><li><code>x(min)</code> is the minimum value of that column, and</li><li><code>x(max)</code> is the maximum value of that column</li></ul><p>Working on the <code>featuresDf</code> created above, we will import <code>MinMaxScaler</code> from the <code>org.apache.spark.ml.feature</code> package. We now have to create an instance of the <code>MinMaxScaler</code>. It will take two parameters: Input column name, and an Output Column name. This object will transform the contents of the input column vectors into a scaled version, and save it into the output column.</p><p>In our case, we will be using our <code>features</code> column inside <code>featuresDf</code> as the input column, and our output column will be named <code>sFeatures</code>. We create the instance in this manner:</p><figure class="kg-card kg-code-card"><pre><code class="language-scala">val featureScaler = new MinMaxScaler()
    .setInputCol(&quot;features&quot;)
    .setOutputCol(&quot;sfeatures&quot;)</code></pre><figcaption>Creating the <code>MinMaxScaler</code> object.</figcaption></figure><p>Next, we have to <code>fit</code> the data present in our <code>featuresDf</code> inside this <code>featureScaler</code> and later <code>transform</code> to create the scaled data. This is done using the code below:</p><figure class="kg-card kg-code-card"><pre><code class="language-scala">val scaledDf = featureScaler.fit(featuresDf)
	.transform(featuresDf)</code></pre><figcaption>Transforming original values into normalized ones</figcaption></figure><p>Now, if we have a look at our transformed data:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://blog.sparker0i.me/content/images/2020/05/image-4.png" class="kg-image" alt="Cool Spark ML - Part 2: Preprocessing of Data" loading="lazy"><figcaption>Normalized <code>DataFrame</code></figcaption></figure><p>You can then use this new <code>sFeatures</code> to calculate distances among points.</p><h3 id="standardize">Standardize</h3><p>Now, we may have data whose values can be mapped to a bell-shaped curve, or normally distributed but maybe not exactly. With standardization, we map our data and transform it, which has a variance of 1 and/or a mean value of 0. This is done because some machine learning algorithms, like SVM, work better this way.</p><p>Thus, what happens is when we apply standardization, our data is slightly shifted in its shape so that it becomes more normalized, or more like a bell curve. The formula to convert values in a non-standardized column to a standardized form is given by:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://blog.sparker0i.me/content/images/2020/05/image-6.png" class="kg-image" alt="Cool Spark ML - Part 2: Preprocessing of Data" loading="lazy"><figcaption>Standardization Formula</figcaption></figure><p>Where:</p><ul><li><code>x</code> is the value to be standardized</li><li><code>x(new)</code> is the standardized value</li><li><code>&#x3BC;</code> is the mean of the column</li><li><code>&#x3C3;</code> is the standard deviation of the column.</li></ul><p>Again, we will be using the <code>featuresDf</code> created above. We will import <code>StandardScaler</code> from the <code>org.apache.spark.ml.feature</code> package. Just like <code>MinMaxScaler</code>, an instance of <code>StandardScaler</code> will require an input column and an output column. In our case, we will still continue with <code>features</code> and <code>sFeatures</code>. We will then <code>fit</code> the data inside the scaler and later <code>transform</code> the data. I&apos;ve combined both these steps into a single code snippet:</p><figure class="kg-card kg-code-card"><pre><code class="language-scala">val featureStandardScaler = new StandardScaler()
    .setInputCol(&quot;features&quot;)
    .setOutputCol(&quot;sfeatures&quot;)
    .setWithStd(true)
    .setWithMean(true)
    
val standardizedDf = featureStandardScaler.fit(featuresDf)
    .transform(featuresDf)</code></pre><figcaption>Standardization of numeric data in Spark ML</figcaption></figure><p>Now if we have a look at our transformed data:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://blog.sparker0i.me/content/images/2020/05/image-8.png" class="kg-image" alt="Cool Spark ML - Part 2: Preprocessing of Data" loading="lazy"><figcaption>Standardized Numeric Data</figcaption></figure><p>Wait, weren&apos;t the values supposed to be scaled within the range of <code>[-1, 1]</code>? Well, that&apos;s the surprise associated with the <code>StandardScaler</code>. It uses the unbiased sample standard deviation instead of the population standard deviation. </p><p>In other words, while the standard deviation will be 1 (or very close to 1), the mean may not be necessarily 0. To scale your data in a way that the range of numbers is between <code>[-1,1]</code> and the standard deviation is 1 and mean 0, you will have to follow <a href="https://stackoverflow.com/a/51755387/2451763?ref=localhost">this accepted StackOverflow answer</a>. Even otherwise with this process, the data has been standardized.</p><h3 id="bucketize">Bucketize</h3><p>Bucketization is done when we have to organize continuous ranges of data into different buckets. <code>Bucketizer</code> allows us to group data based on boundaries, so a list of boundaries has to be provided. I will call it <code>splits</code> with the domain of all buckets when added looks like: <code>{(-&#x221E;, -500.0) &#x22C3; [-500.0, -100.0) &#x22C3; [-100.0, -10.0) &#x22C3; [-10.0, 0.0) &#x22C3; [0.0, 10.0) &#x22C3; [10.0, 100.0) &#x22C3; [100.0, 500.0) &#x22C3; [500.0, &#x221E;)}</code>.</p><p>Then I&apos;ll generate 1000 random points that fall in the range of <code>[-10000.0, 10000.0]</code> and save it in a <code>DataFrame</code> with column name as <code>features</code>. This is done using the below code:</p><pre><code class="language-scala">val splits = Array(Float.NegativeInfinity, -500.0, -100.0, -10.0, 0.0, 10.0, 100.0, 500.0, Float.PositiveInfinity)

val bucketData = (for (i &lt;- 0 to 10000) yield math.random * 10000.0 * (if (math.random &lt; 0.5) -1 else 1))
val bucketDf = bucketData.toDF(&quot;features&quot;)</code></pre><p>Now, our <code>Bucketizer</code> needs three inputs: the splits, input column name, and output column name. Then I&apos;ll <code>transform</code> that data which would then give me the element and which bucket it belongs to:</p><figure class="kg-card kg-code-card"><pre><code class="language-scala">val bucketizer = new Bucketizer()
    .setSplits(splits)
    .setInputCol(&quot;features&quot;)
    .setOutputCol(&quot;bfeatures&quot;)

val bucketedDf = bucketizer.transform(bucketDf)</code></pre><figcaption>Bucketizing Numeric Data in Spark</figcaption></figure><p>Notice that I didn&apos;t have to do a <code>fit</code> operation before doing a <code>transform</code>. This is because Bucketizing is fairly simple and you only need to find which bucket a number belongs to. Thus, there are no operations like scaling which happened in the other 2 sections, and hence you don&apos;t need to <code>fit</code> your data. Now if we have a look at the created <code>DataFrame</code>:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://blog.sparker0i.me/content/images/2020/05/image-9.png" class="kg-image" alt="Cool Spark ML - Part 2: Preprocessing of Data" loading="lazy"><figcaption>Bucketized DataFrame</figcaption></figure><p>Now you might also want to know how many numbers are there in a particular bucket. So, I will do a <code>groupBy</code> on <code>bFeatures</code> column and retrieve the count of occurrences. The following code does that and displays my generated data:</p><figure class="kg-card kg-image-card"><img src="https://blog.sparker0i.me/content/images/2020/05/image-10.png" class="kg-image" alt="Cool Spark ML - Part 2: Preprocessing of Data" loading="lazy"></figure><p>Fairly easy, isn&apos;t it?</p><h2 id="text">Text</h2><p>There are two ways in which you can preprocess text-based data in Spark:</p><ul><li>Tokenize</li><li>TF-IDF</li></ul><p>To illustrate both of them, I will be using <code>sentencesDf</code> created using this code:</p><figure class="kg-card kg-code-card"><pre><code class="language-scala">val sentencesDf = Seq(
    (1, &quot;This is an introduction to Spark ML&quot;),
    (2, &quot;MLLib includes libraries for classification and regression&quot;),
    (3, &quot;It also contains supporting tools for pipelines&quot;)
).toDF(&quot;id&quot;, &quot;sentence&quot;)</code></pre><figcaption>Creating a <code>DataFrame</code> of sentences</figcaption></figure><h3 id="tokenize">Tokenize</h3><p>In tokenization, you map your string containing a sentence into a set of tokens, or words. As an Example, the sentence <em>&quot;This is an introduction to Spark ML&quot;</em> can be mapped into a list of 7 words - <code>{This, is, an, introduction, to, Spark, ML}</code>.</p><p>We will first import <code>Tokenizer</code> from the <code>org.apache.spark.ml.feature</code> package. Now an instance of this will need two parameters - input column and output column. Our input will be <code>sentence</code> and the output will be <code>words</code>, because that is what the <code>Tokenizer</code> will produce. Then we will apply <code>transform</code> on the sentences above.</p><p>Now, just like bucketing, we are not <code>fit</code>ting any data here. <code>Tokenizer</code> already knows its job - Split strings into the separate words. The above process is illustrated in the code below:</p><pre><code class="language-scala">val sentenceToken = new Tokenizer()
    .setInputCol(&quot;sentence&quot;)
    .setOutputCol(&quot;words&quot;)

val sentenceTokenizedDf = sentenceToken.transform(sentencesDf)</code></pre><p>Now, if we have a look at our data:</p><figure class="kg-card kg-image-card"><img src="https://blog.sparker0i.me/content/images/2020/05/image-11.png" class="kg-image" alt="Cool Spark ML - Part 2: Preprocessing of Data" loading="lazy"></figure><p>The <code>words</code> column contains lists of words that have been broken up in the ways you would expect a regular expression pattern matching to break up a sentence into words - based on white space, punctuation, etc. </p><p>Easy, isn&apos;t it?</p><h3 id="term-frequency-inverse-document-frequency-tf-idf-">Term Frequency-Inverse Document Frequency (TF-IDF)</h3><p>Here we map text from a single, typically long string, to a vector, indicating the frequency of each word in a text relative to a group of texts such as a corpus. This transformation is widely used in text classification. </p><figure class="kg-card kg-image-card"><img src="https://blog.sparker0i.me/content/images/2020/05/image-12.png" class="kg-image" alt="Cool Spark ML - Part 2: Preprocessing of Data" loading="lazy"></figure><p>TF-IDF captures the intuition that infrequently used words are more useful for distinguishing categories of text than frequently used words. Considering the above figure as an example, <em>Normalizing </em>appears only once, <em>to</em> appears twice and so on. Like this, we go through all the documents in our corpus, which is nothing but a collection of documents. Then we count up how often a term appears across all of the documents. In this example <em>normalizing </em>is a very rare word. Whereas other words like <em>maps, data</em> and <em>to</em> show up more frequently. We use these two sets of counts and feed those two into the term frequency-inverse document frequency calculation. And that gives us our TF-IDF measures.</p><p>I will use the same <code>sentenceTokenizedDf</code> created above for this exercise as well. Just like other processes mentioned above, we will need to import a few things from <code>org.apache.spark.ml.feature</code> package - <code>HashingTF</code> (for hashing Term Frequency), <code>IDF</code> (for Inverse Document Frequency), <code>Tokenizer</code>.</p><p>First, I will create a <code>HashingTF</code> instance - which takes an input column (<code>words</code>), an output column (<code>rawFeatures</code>) &#xA0;and the number of features to keep track of (<code>20</code>) as the parameters. Now we apply our <code>transform</code>ation on this and get a new <code>DataFrame</code>:</p><pre><code class="language-scala">val hashingTF = new HashingTF()
    .setInputCol(&quot;words&quot;)
    .setOutputCol(&quot;rawFeatures&quot;)
    .setNumFeatures(20)

val sentenceHashingFunctionTermFrequencyDf = hashingTF.transform(sentenceTokenizedDf)
sentenceHashingFunctionTermFrequencyDf.show()</code></pre><p>Now if we have a look at our data, it has added an extra column which is of <code>Vector</code> type. It has mapped each word to an index, so for example, <em>this</em> maps to 1, <em>is </em>maps to 4, <em>an </em>-&gt; 5, and so on. </p><figure class="kg-card kg-image-card"><img src="https://blog.sparker0i.me/content/images/2020/05/image-13.png" class="kg-image" alt="Cool Spark ML - Part 2: Preprocessing of Data" loading="lazy"></figure><p>Now we&apos;re going to scale the <code>rawFeatures</code> vector values and we&apos;re going to scale them based on how often the words appear in the entire collection of sentences. To do this we&apos;re going to create an <code>IDF</code> instance. Again, we have to specify an input column (<code>rawFeatures</code>) and an output column (<code>idfFeatures</code>) as parameters.</p><p>Let&apos;s use the term frequency data we just calculated to <code>fit</code> the inverse document frequency model. And to do that I&apos;m going to create an <code>idfModel</code>, and we&apos;re going to call the <code>idf</code> object I just created, and I&apos;m going to fit it using our term frequency data. Then we apply the IDF <code>transform</code>ation to create a new <code>DataFrame</code> that has both the term frequency and the inverse document frequency transformations applied.</p><pre><code class="language-scala">val idf = new IDF()
    .setInputCol(&quot;rawFeatures&quot;)
    .setOutputCol(&quot;idfFeatures&quot;)

val idfModel = idf.fit(sentenceHashingFunctionTermFrequencyDf)
val tfIdfDf = idfModel.transform(sentenceHashingFunctionTermFrequencyDf)</code></pre><p>Now if we have a look at our data (I&apos;m selecting only the <code>rawFeatures</code> and <code>idfFeatures</code> columns to fit in the screen):</p><figure class="kg-card kg-image-card"><img src="https://blog.sparker0i.me/content/images/2020/05/image-14.png" class="kg-image" alt="Cool Spark ML - Part 2: Preprocessing of Data" loading="lazy"></figure><p>Now we have a new column which contains the inverse document frequency features. These are measures of each word relative to how frequently they occur in the entire corpus. In our case our corpus is just three sentences.</p><h2 id="conclusion">CONCLUSION</h2><p>Preprocessing is indeed a tough challenge where you will have to know what kinds of data you might get and what kinds of processing you want to apply on your data. If not done properly, your machine learning models might not be of much use.</p>]]></content:encoded></item><item><title><![CDATA[My journey to 10k post views on LinkedIn]]></title><description><![CDATA[My LinkedIn posts' Views and Reaction Count went up by 100x and 20x respectively. See my experiences here.]]></description><link>https://blog.sparker0i.me/journey-10k-post-views-linkedin/</link><guid isPermaLink="false">61f016ac0598f2d200c5e15c</guid><category><![CDATA[LinkedIn]]></category><dc:creator><![CDATA[Aaditya Menon]]></dc:creator><pubDate>Mon, 18 May 2020 19:02:45 GMT</pubDate><media:content url="https://blog.sparker0i.me/content/images/2020/05/Untitled.jpg" medium="image"/><content:encoded><![CDATA[<img src="https://blog.sparker0i.me/content/images/2020/05/Untitled.jpg" alt="My journey to 10k post views on LinkedIn"><p>&quot;Going Viral&quot; and &quot;Trending&quot; are probably two words which are a big deal in today&apos;s world and use networking platforms to connect and grow.</p><p>It&apos;s an even bigger deal for a software developer like me who just started a career almost 1.5 years back. I would have never thought that one of my posts on LinkedIn would ever be trending.</p><p>But it has happened, and here I am putting out my story.</p><h2 id="how-did-i-perform-on-linkedin-earlier">How did I perform on LinkedIn earlier?</h2><p>Before May 2020 started, I used to have around 200 connections. Whenever I used to share something on my LinkedIn profile, it would mostly go unnoticed.</p><p>Views would never cross the double-digit mark. Likes were usually single digit numbers, if not 0. 0 Comments. Period.</p><p>Whereas, the people I was following, had a lot of likes and comments on their posts. When I had a look at their posts, their content was very well structured. Their posts were written as if they were telling a story. Then I realized they used to like and comment on these posts because they were able to relate themselves to it.</p><p>Later, when I had a look at my posts on LinkedIn, they never had content. They used to be mostly reshares of someone else&apos;s content. You can see a few examples below, along with the view count of that post:</p><figure class="kg-card kg-gallery-card kg-width-wide kg-card-hascaption"><div class="kg-gallery-container"><div class="kg-gallery-row"><div class="kg-gallery-image"><img src="https://blog.sparker0i.me/content/images/2020/05/Annotation-2020-05-18-220045.png" width="551" height="743" loading="lazy" alt="My journey to 10k post views on LinkedIn"></div><div class="kg-gallery-image"><img src="https://blog.sparker0i.me/content/images/2020/05/Annotation-2020-05-18-220125.png" width="552" height="650" loading="lazy" alt="My journey to 10k post views on LinkedIn"></div><div class="kg-gallery-image"><img src="https://blog.sparker0i.me/content/images/2020/05/Annotation-2020-05-18-220257.png" width="550" height="728" loading="lazy" alt="My journey to 10k post views on LinkedIn"></div></div><div class="kg-gallery-row"><div class="kg-gallery-image"><img src="https://blog.sparker0i.me/content/images/2020/05/Annotation-2020-05-18-220108-1.png" width="554" height="572" loading="lazy" alt="My journey to 10k post views on LinkedIn"></div><div class="kg-gallery-image"><img src="https://blog.sparker0i.me/content/images/2020/05/Annotation-2020-05-18-220330-1.png" width="550" height="405" loading="lazy" alt="My journey to 10k post views on LinkedIn"></div></div></div><figcaption>How my LinkedIn posts looked like before.</figcaption></figure><h2 id="what-triggered-a-change">What triggered a change?</h2><p>Around March 2020, I saw an amazing article on a website which gave a glimpse on the new features coming to Spark 3.0 (You can find that post <a href="https://towardsdatascience.com/glimpse-into-spark-3-0-early-access-c1854327d6c?ref=localhost">here</a>). This time around, while sharing this post on LinkedIn, I decided to put in my thoughts on what I felt about the release of Spark 3. Find this post below:</p><!--kg-card-begin: markdown--><iframe src="https://www.linkedin.com/embed/feed/update/urn:li:share:6645553272264491009" height="376" width="504" frameborder="0" allowfullscreen title="Embedded post"></iframe><!--kg-card-end: markdown--><p>This was the first time the View count on any of my posts stepped into the triple-figure mark. This was also the first time the Reaction count on my posts touched 5 (the previous best at this point was 3, as shown in the screenshot above).</p><p>Because there&apos;s always a first time for everything ;)</p><p>This was the time I realized that if I put my thoughts on what I felt about something, I would surely gain more views and likes. I definitely wanted to grow more on LinkedIn and realized that I had to put my content in a better way that people don&apos;t find crowded. (That embedded post above had only 1 paragraph which felt cluttered).</p><h2 id="the-guru">The Guru</h2><p>Around a month later, I saw one of my connections, Vaibhav Sisinty commenting on people&apos;s posts about his 5-Day LinkedIn workshop. That&apos;s not just once. I saw his comments on multiple people&apos;s posts.</p><p>While reading the posts, I actually felt that this isn&apos;t promotion going on. People are telling their experiences out of the workshop. People were telling the steps they executed and were also showing the results they achieved.</p><p>The cost of the workshop was a measly Rs. 500. It isn&apos;t a huge amount where we would have to empty our pockets to grow. I actually contemplated on whether to join his workshop or not a lot. I was in a dilemma because I was just another (inexperienced) software developer on LinkedIn, amongst 1000s of others. I also had other thoughts on my mind - whether this will workshop will help me as a software developer or not.</p><p>Then I took a leap of faith... and joined his 5-Day LinkedIn Workshop.</p><h2 id="the-workshop">The workshop</h2><p>I was a part of Batch 3 of his workshop, which ran from 5th-11th May. In short, the content of the workshop was divided into a few parts, each taken over a period of 5 days (actually 6, because he gave one extra day off to complete one of the tasks):</p><ul><li>Understanding Target Audience</li><li>Optimizing LinkedIn profile</li><li>Connect with the target audience</li><li>Create good content</li></ul><p>The content for each of those points in the workshop was well defined. He had recorded sessions for each of those 5 days, and at 9PM he used to have a Live Q&amp;A session of that day&apos;s topic. Despite all of the above, execution is key. </p><p>&quot;No Execution, No Results&quot;. </p><p>This is one of the golden words Vaibhav used to frequently tell inside his videos and the Facebook and WhatsApp groups {Yes, he used to engage with us on WhatsApp as well ;) }. He would also send reminder emails (I guess he sent around 50-70 such mails) to me (and I guess each participant).</p><h2 id="the-content-boom">The content boom</h2><p>Ever since the workshop, I posted few contents, whose views and Reaction count were significantly higher than any of my previous posts. Have a look at the two screenshots below:</p><figure class="kg-card kg-gallery-card kg-width-wide kg-card-hascaption"><div class="kg-gallery-container"><div class="kg-gallery-row"><div class="kg-gallery-image"><img src="https://blog.sparker0i.me/content/images/2020/05/Annotation-2020-05-18-235301.png" width="553" height="549" loading="lazy" alt="My journey to 10k post views on LinkedIn"></div><div class="kg-gallery-image"><img src="https://blog.sparker0i.me/content/images/2020/05/Annotation-2020-05-18-235343.png" width="496" height="859" loading="lazy" alt="My journey to 10k post views on LinkedIn"></div></div></div><figcaption>Views and Reaction count on my posts after executing the contents of the workshop</figcaption></figure><p>At least 3x the usual reaction counts and 5-8x views of my posts. I was definitely improving in my LinkedIn game.</p><p>Profile views and search appearances were up as well:</p><figure class="kg-card kg-gallery-card kg-width-wide kg-card-hascaption"><div class="kg-gallery-container"><div class="kg-gallery-row"><div class="kg-gallery-image"><img src="https://blog.sparker0i.me/content/images/2020/05/Annotation-2020-05-11-121331-1.png" width="838" height="354" loading="lazy" alt="My journey to 10k post views on LinkedIn" srcset="https://blog.sparker0i.me/content/images/size/w600/2020/05/Annotation-2020-05-11-121331-1.png 600w, https://blog.sparker0i.me/content/images/2020/05/Annotation-2020-05-11-121331-1.png 838w" sizes="(min-width: 720px) 720px"></div><div class="kg-gallery-image"><img src="https://blog.sparker0i.me/content/images/2020/05/Annotation-2020-05-19-000057.png" width="835" height="331" loading="lazy" alt="My journey to 10k post views on LinkedIn" srcset="https://blog.sparker0i.me/content/images/size/w600/2020/05/Annotation-2020-05-19-000057.png 600w, https://blog.sparker0i.me/content/images/2020/05/Annotation-2020-05-19-000057.png 835w" sizes="(min-width: 720px) 720px"></div></div></div><figcaption>Profile views after executing contents of the workshop</figcaption></figure><p>Then came the blockbuster. On May 15th 2020 at around 10:15pm, I put up a post on LinkedIn regarding native GPU support for Spark. This was a recent news, and Spark was something I&apos;m working on at my workplace. Posting my thoughts on that news definitely made sense. Have a look at that post below:</p><!--kg-card-begin: markdown--><iframe src="https://www.linkedin.com/embed/feed/update/urn:li:share:6667102362034933760" height="775" width="504" frameborder="0" allowfullscreen title="Embedded post"></iframe><!--kg-card-end: markdown--><p>This was the post where I&apos;ve got the best results on inside LinkedIn. Look at the counts below. These numbers are insane for a software developer who had just started out his career:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://blog.sparker0i.me/content/images/2020/05/image-1.png" class="kg-image" alt="My journey to 10k post views on LinkedIn" loading="lazy"><figcaption>View and Reaction count for the above post</figcaption></figure><p>Oh yes, and the &quot;Trending on LinkedIn&quot; happened as well, 2.5 days after putting up that post:</p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://blog.sparker0i.me/content/images/2020/05/Annotation-2020-05-18-123138.png" class="kg-image" alt="My journey to 10k post views on LinkedIn" loading="lazy"><figcaption>The Trending part</figcaption></figure><p>Couldn&apos;t expect more for myself.</p><h2 id="conclusion">Conclusion</h2><p>While I did start to see great numbers after executing contents in the workshop, complacency isn&apos;t allowed. The workshop contents are supposed to be executed for long (unless LinkedIn detects you are a Super User and stops you from executing for the rest of the day).</p><p>Vaibhav&apos;s workshop is a great place to get started whoever you are - from an entry level Software Developer, to a Digital Marketer, to someone finding the next job - this workshop is for everyone who wants to improve their LinkedIn profile.</p><p>Do visit <a href="https://www.linkedin.com/in/vaibhavsisinty/?ref=localhost">Vaibhav&apos;s profile on LinkedIn</a>.</p><p>Until another post, ciao.</p><p><em>(PS. Not a promotional post. These are definitely my experiences out of this workshop. You can see the results for yourselves)</em></p>]]></content:encoded></item><item><title><![CDATA[Spark 3.0 adds native GPU integration: Why that matters?]]></title><description><![CDATA[You won't have to depend on your CPU anymore to run Spark based jobs. You will be able to offload them off to the GPU. Learn why this matters in this post]]></description><link>https://blog.sparker0i.me/spark-3-native-gpu-integration/</link><guid isPermaLink="false">61f016ac0598f2d200c5e15b</guid><category><![CDATA[Spark]]></category><category><![CDATA[Spark ML]]></category><category><![CDATA[Nvidia]]></category><dc:creator><![CDATA[Aaditya Menon]]></dc:creator><pubDate>Sat, 16 May 2020 18:01:34 GMT</pubDate><media:content url="https://blog.sparker0i.me/content/images/2020/05/NVIDIA-Accelerates-Apache-Spark.jpg" medium="image"/><content:encoded><![CDATA[<img src="https://blog.sparker0i.me/content/images/2020/05/NVIDIA-Accelerates-Apache-Spark.jpg" alt="Spark 3.0 adds native GPU integration: Why that matters?"><p>You can soon run your Apache Spark programs natively on your GPU. This became possible thanks to collaboration between Nvidia and Databricks. At the GPU Technology Conference, both the companies have presented a solution that brings GPU Acceleration to Spark 3.0 without major code changes.</p><h2 id="how-things-were-before">How things were before?</h2><p>GPU based solutions have existed for Spark for a long time, so what has changed?</p><p>Such GPU integrations into Spark were provided by either third party libraries in Java/Scala, or you had to depend on Cloud Providers which would provide such an infrastructure to run Spark on GPU. Also, programs would usually be restricted to applications based on Spark ML, thus they generally couldn&apos;t be applied to other Big Data uses on Scale.</p><p>When it comes to Spark/Python, you had to use custom tools like Horovod, which would also end up using popular Python based libraries like Numpy and Tensorflow. Thus, this approach severely limits the performance of the Spark Programs due to the nature of Python, where programs are dynamically interpreted. </p><p>Don&apos;t get me wrong, Python has its very own unique use-cases which Scala doesn&apos;t provide (yet), but because Spark was built to do Big Data operations effectively, Python severely restricts the performance.</p><h2 id="what-happened-now">What happened now?</h2><p>With the release of Spark 3.0, native GPU based acceleration will be provided within Spark. This acceleration is based on the open source <a href="https://www.anrdoezrs.net/links/9041660/type/dlg/sid/zd-ad14a02e5d404bd4822065953dda157b--%7Cxid:fr1589641152241fef/https://developer.nvidia.com/rapids?ref=localhost" rel="noopener noreferrer nofollow">RAPIDS</a> suite of software libraries, Nvidia built on <a href="https://www.anrdoezrs.net/links/9041660/type/dlg/sid/zd-ad14a02e5d404bd4822065953dda157b--%7Cxid:fr1589641152241dce/https://developer.nvidia.com/machine-learning?ref=localhost" rel="noopener noreferrer nofollow">CUDA-X AI</a>. This will allow developers to run Spark code without any modifications on GPUs - thereby alleviating load off the CPU.</p><p>This also benefits Spark SQL and <code>DataFrame</code> operations, thereby making the GPU acceleration benefits available for non-Machine Learning workloads as well. This will also bring capabilities where we don&apos;t have to provision a dedicated Spark Cluster for AI and ML based jobs.</p><p>In an advanced briefing for members of the press, NVidia CEO Jensen Huang explained that users of Spark clusters on <a href="https://click.linksynergy.com/deeplink?id=IokOf8qagZo&amp;mid=24542&amp;u1=zd-ad14a02e5d404bd4822065953dda157b--%7Cxid%3Afr1589641152241ghb&amp;murl=https%3A%2F%2Fazure.microsoft.com%2Fservices%2Fmachine-learning%2F&amp;ref=localhost" rel="noopener noreferrer nofollow">Azure Machine Learning</a> or <a href="https://aws.amazon.com/sagemaker/?ref=localhost" rel="noopener noreferrer nofollow">Amazon SageMaker</a> can benefit from the GPU acceleration as well. This means that the infrastructure is already in place, it is now upon other cloud providers to provide the necessary infrastructure, and upon developers to adopt and build their workloads to the new changes.</p><h2 id="adobe-spark-gpu-acceleration">Adobe + Spark GPU Acceleration</h2><p>Adobe and Nvidia had signed a <a href="https://news.adobe.com/news/news-details/2018/Adobe-and-NVIDIA-Announce-Partnership-to-Deliver-New-AI-Services-for-Creativity-and-Digital-Experiences/default.aspx?ref=localhost">deal</a> in 2018 where they will utilize Nvidia&apos;s AI capabilities for their solutions. Building upon this deal, Adobe has been an early adopter for this new GPU Acceleration on Spark, and they have shown a 7x improvement in performance of their workloads, while saving up to 90% of the costs. </p><p>These are serious numbers. Imagine, if a company as huge as Adobe is able to bring down costs while improving performance, other companies too can follow suit and we could see Profits and Performance for everyone. Period.</p><h2 id="conclusion">Conclusion</h2><p>Imagine how game changing this can prove to be for the Big Data community overall. No longer will we have to wait for operations to complete when we can utilize the GPU, we have on our local Gaming PCs and laptops. We will also be able to utilize GPU servers on Cloud for Spark without doing major changes.</p><p>This can also encourage many people to start using Scala for AI and Machine Learning instead of Python. While I do realize that there are no major visualization libraries supporting Spark available in Scala, an encouragement to do machine learning with Spark shall bring more enthusiasm for Scala, due to the disadvantages I mentioned for Python above. This in turn will lead to a growth in the Scala community, which will further result in availability of more and more libraries. </p><p>For now, there is a Scala visualization library that supports Spark, in active development, which when released to MVN Repository could be a game changer. Head over to <a href="https://github.com/MarkCLewis/SwiftVis2?ref=localhost">SwiftViz2&apos;s GitHub repo</a> for more info. You can place safe bets on this one :)</p><p>In short, this is a win-win situation for everyone involved in this ecosystem.</p><p>Until another blog post, Ciao.</p><h3 id="sources">SOURCES</h3><p><a href="https://www.zdnet.com/article/nvidia-and-databricks-announce-gpu-acceleration-for-spark-3-0/?ref=localhost">ZdNet</a>, <a href="https://nvidianews.nvidia.com/news/nvidia-accelerates-apache-spark-worlds-leading-data-analytics-platform?ref=localhost">Nvidia Newsroom</a></p>]]></content:encoded></item><item><title><![CDATA[Tail Recursion: Why and How-to Use in Scala]]></title><description><![CDATA[Read my blog post to know more about the advantages of tail recursion, and how do you use it in Scala.]]></description><link>https://blog.sparker0i.me/tail-recursion-scala-why-how-to/</link><guid isPermaLink="false">61f016ac0598f2d200c5e157</guid><category><![CDATA[Scala]]></category><dc:creator><![CDATA[Aaditya Menon]]></dc:creator><pubDate>Thu, 07 May 2020 19:45:00 GMT</pubDate><media:content url="https://blog.sparker0i.me/content/images/2020/05/imgonline-com-ua-Transparent-background-I9SFpWZMbxlkM67n.png" medium="image"/><content:encoded><![CDATA[<img src="https://blog.sparker0i.me/content/images/2020/05/imgonline-com-ua-Transparent-background-I9SFpWZMbxlkM67n.png" alt="Tail Recursion: Why and How-to Use in Scala"><p>In the below code, I have written a recursive function that multiplies all the natural numbers up to the number passed as a parameter to the function. As you might have guessed, this is nothing but computing the factorial of a particular number.</p><figure class="kg-card kg-code-card"><pre><code class="language-scala">def recursiveProd(x: Int): BigInt = {
    if (x &lt;= 1) 
        return 1
    else 
        return x * recursiveProd(x-1)
}</code></pre><figcaption>Recursive Factorial Program</figcaption></figure><p>Let us see how this function is being executed as a whole assuming we executed <code>recursiveProd(5)</code>:</p><figure class="kg-card kg-code-card"><pre><code>recursiveProd(5)  
5 * recursiveProd(4)  
    (4 * recursiveProd(3))  
         (3 * recursiveProd(2))
              (2 * recursiveProd(1))  
                   1 
120</code></pre><figcaption>Recursive Factorial Steps</figcaption></figure><p>From above, each recursive call has to be completed first before the actual work of calculating the product begins. Each recursive call saves the current state, and proceeds to call the next recursive function. This happens repeatedly until the base case is reached. In between, you might also encounter the Stack Overflow error. </p><p>So, in each step you execute 2 steps, retrieve the current value and the value from the next stage (as a recursive call), and then multiply them. Subsequent recursive calls will do the same. If you can visualize this correctly, you will notice this recursive call was completed in <strong>14 computations </strong>(4 multiplications, 5 recursive calls, 5 returning values), with computations happening in each step.</p><h3 id="tail-recursion">Tail Recursion</h3><p>Now let&#x2019;s consider Tail Recursion. In Tail Recursion, all the processing related to the recursive function must finish before the recursive call takes place. This means that <strong>if a function is tail-recursive, the last action is a call to itself</strong>.</p><figure class="kg-card kg-code-card"><pre><code class="language-scala">def tailRecursiveProd(x: Int, currentTotal: BigInt): BigInt = {
    if (x &lt;= 1) 
        return currentTotal
    else 
        return tailRecursiveProd(x - 1, currentTotal * x)
}</code></pre><figcaption>Tail-Recursive Factorial Program</figcaption></figure><p>In this scenario, despite there being a multiplication operation, it happens when the argument is passed to the next recursive call. In short, we send the current state of the recursive call to the next state, and the same process will be repeated until the base case is reached. Let us see how this is executed:</p><figure class="kg-card kg-code-card"><pre><code class="language-scala">recursiveProd(5,1)
recursiveProd(4,5)
recursiveProd(3,20)
recursiveProd(2,60)
recursiveProd(1,120)
120</code></pre><figcaption>Tail-Recursive Factorial Steps</figcaption></figure><p>In this way, we can save up additional stack memory which would&apos;ve otherwise be wasted to compute the multiplications at every return step. Thus, this implementation only takes 10 computations (5 recursive calls, 5 returning values). This is equivalent of you using a loop to process the factorial.</p><p>Thus, you should always try and convert your recursive function into a tail recursive function wherever possible.</p><h3 id="tail-recursion-in-scala">Tail Recursion in Scala</h3><p>One good thing about Scala is that it automatically recognizes two types of tail-recursive methods automatically and optimizes them. These types are:</p><ol><li>Methods within an <code>object</code></li><li>Methods defined as <code>final</code></li></ol><p>Sadly, if you write a non-<code>final</code> tail-recursive function inside a <code>class</code>, or even a <code>case class</code>, it will not be automatically optimized by the Scala Compiler because a <code>class</code> can be <code>extend</code>ed and these methods can be <code>override</code>n. Consider my code given below:</p><figure class="kg-card kg-code-card"><pre><code class="language-scala">object Bm {
    def nTailRecursion(n: Int): Int = {
        if (n == 0) 1 else nTailRecursion(n - 1)
    }
}

case class Bm() {
    def tailRecursion(n: Int): Int = {
        if (n == 0) 1 else tailRecursion(n - 1)
    }

    final def tailsRecursion(n: Int): Int = {
        if (n == 0) 1 else tailsRecursion(n - 1)
    }
}</code></pre><figcaption>Illustrating various cases of tail-recursive methods</figcaption></figure><p>You can see that all these functions are doing the same task. Now:</p><ol><li>Start a Scala REPL (Install Scala on your machine, then type <code>scala</code> on your command line/terminal and press Enter)</li><li>Type <code>:paste</code> and press Enter</li><li>Paste the code snippet above</li><li>Press <code>Ctrl-D</code> to exit the paste mode</li></ol><p>Then, try running <code>Bm.nTailRecursion(60000)</code> and <code>Bm().tailsRecursion(60000)</code>. I&apos;ve tried that on my current laptop with an Intel i7-8750H processor and 16GB RAM, and both of them worked fine. Now, when you try running <code>Bm().tailRecursion(60000)</code>, you see a familiar <code>java.lang.StackOverflowError</code> which usually occurs with recursive function:</p><figure class="kg-card kg-image-card"><img src="https://blog.sparker0i.me/content/images/2020/05/image.png" class="kg-image" alt="Tail Recursion: Why and How-to Use in Scala" loading="lazy"></figure><p>Sure, you could play around with the JVM memory limits and possibly execute this function properly. You must always remember that memory is an intensive resource, and non-availability of memory might crash other programs, as well as your current program.</p><p>Fortunately, Scala provides the <code>@tailrec</code> annotation to denote that a method is actually tail-recursive. First you will have to import <code>scala.annotation.tailrec</code> and place that annotation before the function you want to mark as tail-recursive. Place this annotation before <code>tailRecursion()</code> inside the <code>case class</code> and now copy-paste inside the REPL and try again. This time it won&apos;t throw the dreaded <code><code>java.lang.StackOverflowError</code></code> Exception.</p><h3 id="convert-a-recursive-function-to-a-tail-recursive-function">Convert a recursive function to a tail-recursive function</h3><p>In some cases, you might want to retain the original method&apos;s signature (eg. Factorial). This can be done using the following steps:</p><p>1. Create a second function</p><p>Within the <code>recursiveProd</code> as defined in the first code piece above, we now define another method, <code>cumulativeRecursion</code> with two parameters: <code>n</code>, our number and <code>res</code>, the result of recursion. We retain the algorithm of the first method as is. At this point our new method looks like:</p><pre><code class="language-scala">def recursiveProd(n: Int): Int = {
    def cumulativeRecursion(n: Int, res: Int): Int = {
        if (n &lt;= 1) 1
        else n * recursiveProd(n - 1)
    }
}</code></pre><p>2. Modify the second method&apos;s algorithm</p><p>We will now utilize the accumulator we&apos;ve just created, <code>res</code> and modify the function such that the base case returns the accumulated value and the other case recursively calls the new method again:</p><pre><code>def recursiveProd(n: Int): Int = {
    def cumulativeRecursion(n: Int, res: Int): Int = {
        if (n &lt;= 1) res
        else cumulativeRecursion(n - 1, res * n)
    }
}</code></pre><p>3. Annotate the second method and call the new method</p><p>We will now annotate our new method with <code>@tailrec</code> as shown earlier and we will now call this method from our original method:</p><pre><code class="language-scala">def recursiveProd(n: Int): Int = {
    @tailrec def cumulativeRecursion(n: Int, res: Int): Int = {
        if (n &lt;= 1) res
        else cumulativeRecursion(n - 1, res * n)
    }
    cumulativeRecursion(n, 1)
}</code></pre><p>Hence, you retain your method&apos;s original signature, as well as converted it into a tail-recursive call (Though this will add 1 extra stack call to the new function).</p><h3 id="conclusion">CONCLUSION</h3><p>In this post, I have:</p><ul><li>Defined Tail Recursion</li><li>Introduced <code>@tailrec</code> annotation</li><li>Shown a formula to convert a recursive function into a tail-recursive one.</li></ul><p>Hope you have enjoyed this post. Do follow my profiles on <a href="https://www.linkedin.com/in/sparker0i?ref=localhost">LinkedIn</a>, <a href="https://github.com/Sparker0i?ref=localhost">GitHub</a> and <a href="https://twiiter.com/Sparker0i?ref=localhost">Twitter</a>.</p><p>Ciao, until the next post.</p><p>Reference: <a href="https://alvinalexander.com/scala/fp-book/tail-recursive-algorithms/?ref=localhost">Tail Recursive Algorithms</a></p>]]></content:encoded></item><item><title><![CDATA[Cool Spark ML: K Nearest Neighbors]]></title><description><![CDATA[Learn how you can write a KNN algorithm from scratch and modify it for use with larger datasets in Spark]]></description><link>https://blog.sparker0i.me/spark-machine-learning-knn/</link><guid isPermaLink="false">61f016ac0598f2d200c5e159</guid><category><![CDATA[Spark]]></category><category><![CDATA[Spark ML]]></category><category><![CDATA[Scala]]></category><category><![CDATA[Big Data]]></category><category><![CDATA[Machine Learning]]></category><dc:creator><![CDATA[Aaditya Menon]]></dc:creator><pubDate>Sun, 19 Apr 2020 05:41:03 GMT</pubDate><media:content url="https://blog.sparker0i.me/content/images/2020/04/mllib-1.jpg" medium="image"/><content:encoded><![CDATA[<img src="https://blog.sparker0i.me/content/images/2020/04/mllib-1.jpg" alt="Cool Spark ML: K Nearest Neighbors"><p><em>Note: This article is the first of the Series: Cool Spark ML. Other parts are coming soon.</em></p><p>I had taken up a few machine learning courses in my college throughout 2018. Most of the problems there were solved using Python and the necessary libraries - NumPy, Pandas, Scikit-Learn and Matplotlib. With my daily work at IBM now requiring me to use Scala and Spark, I decided to use my free time during the lockdown to try out Spark ML.</p><p><em>Note: All the codes in the Cool Spark ML Series will be available on <a href="https://github.com/Sparker0i/Cool-Spark-ML?ref=localhost">my GitHub repo</a></em></p><h3 id="intro-to-spark-ml">Intro to Spark ML</h3><p>As the name suggests, Spark ML is the Machine Learning library consisting of common Machine learning algorithms - classification, regression, clustering etc.</p><h3 id="why-spark-ml">Why Spark ML?</h3><p>Pandas - a Python library - won&#x2019;t work every time. It is a single machine tool, so it&apos;s constrained by the machine&apos;s limits. Moreover, pandas doesn&#x2019;t have any parallelism built in, which means it uses only one CPU core. You may hit a dead-end on datasets of the size of a few gigabytes. Pandas won&apos;t help if you want to work on very big datasets.</p><p>We are now in the Big Data era, where gigabytes of data are generated every few seconds. Such datasets will require powerful systems to run even the basic machine learning algorithms. The cost of getting such a powerful system will be huge, as well as the costs to scale them up. With distributed computers, such calculations can be sent to multiple low-end machines, which prevents the cost of getting a single high-end machine.</p><p>This is where Spark kicks in. Spark has the concept of <code>DataFrame</code> (now deprecated in favor of Datasets), which behaves very similar to how a Pandas <code>DataFrame</code> would do, including having very similar APIs too. The advantage of using Spark <code>DataFrame</code> is that it was designed from ground-up to support Big Data. Spark can also distribute such <code>DataFrame</code>s across multiple machines and collect the calculated results.</p><h3 id="knn-k-nearest-neighbors">KNN: K-Nearest Neighbors</h3><p>The process in KNN is pretty simple. You load your entire dataset first, each of which will have input columns and one output column. This is then split into a training set and a testing set. You then use your training set to train your model, and then use the testing set to predict the output column value by testing it against the model. You then compare the actual and the predicted target values and calculate the accuracy of your model.</p><h3 id="problem-definition">Problem Definition</h3><p>We are going to train a model to predict the famous <a href="http://archive.ics.uci.edu/ml/datasets/iris?ref=localhost">Iris dataset</a>. The Iris Flower Dataset involves predicting the flower species given measurements of iris flowers.</p><p>It is a multiclass classification problem. The number of observations for each class is the same. The dataset is small in size with only 150 rows with 4 input variables and 1 output variable.</p><p>The 4 features are described as follows:</p><ol><li>Sepal-Length, in cm</li><li>Sepal-Width, in cm</li><li>Petal-Length, in cm</li><li>Petal-Width, in cm</li></ol><h3 id="prerequisites">Prerequisites</h3><ol><li>Create a Scala project in IntelliJ IDEA based on SBT</li><li>Select Scala version 2.11.12</li><li>Include <code>spark-core</code>, <code>spark-sql</code> and <code>spark-ml</code> 2.4.5 as library dependencies in your <code>build.sbt</code></li></ol><h3 id="knn-steps">KNN Steps</h3><p>In this blog post, I will be developing KNN algorithm from scratch. The process to perform KNN can be broken down into 3 easy steps:</p><ol><li>Calculate Euclidean Distance</li><li>Get Nearest Neighbors</li><li>Make Predictions</li></ol><h3 id="step-1-calculate-euclidean-distance">Step 1: Calculate Euclidean Distance</h3><p>The first step will be to calculate the distance between two rows in a Dataset. Rows of data are mostly made up of numbers and an easy way to calculate the distance between two rows or vectors of numbers is to draw a straight line.</p><p>Euclidean Distance is calculated as the square root of the sum of the squared differences between the two vectors, as given in the image below:</p><!--kg-card-begin: html--><a href="https://www.codecogs.com/eqnedit.php?latex=%5Cinline&amp;space%3B%5Cbg_white=&amp;space%3B%7B%5Ccolor%7BRed%7D=&amp;space%3B%24%24dist_%7Bx_1%2Cx_2%7D=&amp;space%3B=&amp;space%3B%5Csqrt%7B%5Csum_%7Bi=0%7D%5E%7BN%7D&amp;space%3B%28%7Bx_1_i=&amp;space%3B-=&amp;space%3Bx_2_i%7D%29%5E2%7D.%24%24%7D=&amp;ref=localhost" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\inline&amp;space;\bg_white&amp;space;{\color{Red}&amp;space;$$dist_{x_1,x_2}&amp;space;=&amp;space;\sqrt{\sum_{i=0}^{N}&amp;space;({x_1_i&amp;space;-&amp;space;x_2_i})^2}.$$}" title="{\color{Red} $$dist_{p_1,p_2} = \sqrt{\sum_{i=0}^{N} ({x_1_i - x_2_i})^2}.$$}" alt="Cool Spark ML: K Nearest Neighbors"></a><!--kg-card-end: html--><p>Where <code>x1</code> is the first row of data, <code>x2</code> is the second row of data, and <code>i</code> is a specific index for a column as we sum across all columns. Smaller the value, more similar will be the two rows.</p><p>Since we will be reading our data and transforming it using Spark, to compute distances between two <code>Row</code>s in a <code>DataFrame</code>, we write the function below in Scala:</p><figure class="kg-card kg-code-card"><pre><code class="language-scala">def computeEuclideanDistance(row1: Row, row2: Row): Double = {
    var distance = 0.0
    for (i &lt;- 0 until row1.length - 1) {
        distance += math.pow(row1.getDouble(i) - row2.getDouble(i), 2)
    }
    math.sqrt(distance)
}</code></pre><figcaption>Code to compute Euclidean Distance</figcaption></figure><p>You can see that the function assumes that the last column in each row is an output value which is ignored from the distance calculation.</p><h3 id="step-2-get-nearest-neighbors">Step 2: Get Nearest Neighbors</h3><p>Neighbors for a new piece of data in the dataset are the k closest instances, as defined by our distance measure. To locate the neighbors for a new piece of data within a dataset we must first calculate the distance between each record in the dataset to the new piece of data. We can do this using our distance function prepared above.</p><p>We can do this by keeping track of the distance for each record in the dataset as a tuple, sort the list of tuples by the distance, and then retrieve the neighbors. The below function does this job in Scala:</p><figure class="kg-card kg-code-card"><pre><code class="language-scala">def getNeighbours(trainSet: Array[Row], testRow: Row, k: Int): List[Row] = {
    var distances = mutable.MutableList[(Row, Double)]()
    trainSet.foreach{trainRow =&gt;
        val dist = computeEuclideanDistance(trainRow, testRow)
        val x = (trainRow, dist)
        distances += x
    }
    distances = distances.sortBy(_._2)
    var neighbours = mutable.MutableList[Row]()

    for (i &lt;- 1 to k) {
        neighbours += distances(i)._1
    }
    neighbours.toList
}</code></pre><figcaption>Retrieve The k nearest neighbors for a new data element</figcaption></figure><h3 id="step-3-make-predictions">Step 3: Make Predictions</h3><p>The most similar neighbors collected from the training dataset can be used to make predictions. In the case of classification, we can return the most represented output value (Class) among the neighbors.</p><p>We would first map the class values to the number of times it appears among the neighbors, then sort the counts in descending order and get the most appeared class value. The below function does exactly that in Scala:</p><figure class="kg-card kg-code-card"><pre><code class="language-scala">def predictClassification(trainSet: Array[Row], testRow: Row, k: Int): String =
{
    val neighbours = getNeighbours(trainSet, testRow, k)
    val outputValues = for (row &lt;- neighbours) yield row.getString(trainSet(0).length - 1)
    outputValues.groupBy(identity)
        .mapValues(_.size)
        .toSeq
        .sortWith(_._2 &gt; _._2)
        .head._1
}</code></pre><figcaption>Make prediction on the class value of a new data element</figcaption></figure><h3 id="apply-the-above-concepts-to-iris-dataset">Apply the above concepts to Iris Dataset</h3><p>We will now apply the concepts above to perform KNN on the Iris Dataset.</p><p>First, we have to load the dataset into the program. This is done using the <code>readCsv</code> function I&apos;ve written below:</p><figure class="kg-card kg-code-card"><pre><code class="language-scala">def readCsv(fileName: String, header: Boolean): DataFrame = {
    spark.read
        .format(&quot;csv&quot;)
        .option(&quot;header&quot;, header)
        .option(&quot;inferSchema&quot;, header)
        .load(fileName)
        .repartition($&quot;Class&quot;)
}</code></pre><figcaption>Code to read a CSV into a DataFrame</figcaption></figure><p>We also have to normalize the data we have. This is because KNN is based on distance between records. Unless data is normalized distance will be incorrectly calculated, because different attributes will not contribute to the distance in a uniform way. &#xA0;Attributes having a larger value range will have an unduly large influence on the distance, because they make greater contribution to the distance. If the dataset requires that some columns be given a greater preference over others, then normalization isn&apos;t recommended, but this is not true in the case of the Iris dataset.</p><p>We use the Z Score Normalization technique. With this, we subtract the mean of the respective column from each cell, and divide that with the standard deviation of that column. <a href="https://towardsdatascience.com/understand-data-normalization-in-machine-learning-8ff3062101f0?ref=localhost">This</a> article describes Data Normalization in good detail.</p><p>The following function does our job:</p><pre><code class="language-scala">def normalizeData(): Unit = {
    df.columns.filterNot(e =&gt; e == &quot;Class&quot;).foreach{col =&gt;
        val (mean_col, stddev_col) = df.select(mean(col), stddev(col))
            .as[(Double, Double)]
            .first()
        df = df.withColumn(s&quot;$col.norm&quot;, ($&quot;$col&quot; - mean_col) / stddev_col)
            .drop(col)
            .withColumnRenamed(s&quot;$col.norm&quot;, col)
    }
}</code></pre><p>As you can see above, we are filtering out the class value, because we will not be using this value to compute the distance. There&apos;s one problem with our approach though, our KNN functions written above assume that the class value will be the last column. In the way we&apos;ve normalized the data, we are dropping the original column, and adding the normalized column in place. This will push the <code>Class</code> column to the beginning. So, I&apos;ve written another function which will move the column back to where it should actually be:</p><pre><code class="language-scala">def moveClassToEnd(): Unit = {
    val cols = df.columns.filterNot(_ == &quot;Class&quot;) ++ Array(&quot;Class&quot;)
    df = df.select(cols.head, cols.tail: _*)
}</code></pre><p>We will evaluate our algorithm using K-fold cross-validation with 5 folds. This means that we will have 150/5 = 30 rows per fold. We will use helper functions <code>evaluateAlgorithm()</code> and <code>accuracyMetric()</code> to evaluate the algorithm for cross-validation and calculate the accuracy of our predictions respectively.</p><p>Since Spark does not allow any of its operations inside a Spark transformation, we will have to perform a <code>collect()</code> on the Train set and Test set <code>DataFrame</code>s every time before passing it to any function. A sample run with <code>k = 3</code> produces the following output:</p><figure class="kg-card kg-image-card"><img src="https://blog.sparker0i.me/content/images/2020/04/KNN-Accuracy.png" class="kg-image" alt="Cool Spark ML: K Nearest Neighbors" loading="lazy"></figure><p>Let&apos;s go one step further and run our program over different values of <code>k</code>. I&apos;m running it for <code>k</code> from <code>1 to 10</code>, and here are some results (this may not be the same everytime): </p><figure class="kg-card kg-image-card kg-card-hascaption"><img src="https://blog.sparker0i.me/content/images/2020/04/image.png" class="kg-image" alt="Cool Spark ML: K Nearest Neighbors" loading="lazy"><figcaption>KNN accuracy for a variety of k values</figcaption></figure><p>You can find the entire code below:</p><!--kg-card-begin: html--><script src="https://gist-it.appspot.com/github/Sparker0i/Cool-Spark-ML/blob/master/src/main/scala/me/sparker0i/machinelearning/classification/KNN.scala"></script><!--kg-card-end: html--><h3 id="conclusion">CONCLUSION</h3><p>While Spark ideally shouldn&apos;t be used smaller datasets like this, you could apply the same thought process and transform this code to use for some larger datasets, and there you will see the magic of Spark over Pandas.</p><p>Inspired heavily from <a href="https://machinelearningmastery.com/tutorial-to-implement-k-nearest-neighbors-in-python-from-scratch/?ref=localhost">this</a> great article.</p>]]></content:encoded></item><item><title><![CDATA[Add new functions to existing classes the Scala way]]></title><description><![CDATA[Why extend classes to add a small functionality, when you could just write an implicit class? Read my post on how you too can take advantage of that in Scala]]></description><link>https://blog.sparker0i.me/scala-add-new-functions-to-existing-class/</link><guid isPermaLink="false">61f016ac0598f2d200c5e15a</guid><category><![CDATA[Scala]]></category><dc:creator><![CDATA[Aaditya Menon]]></dc:creator><pubDate>Fri, 10 Apr 2020 18:09:42 GMT</pubDate><media:content url="https://blog.sparker0i.me/content/images/2020/04/1_ygv4NHtC9Jao-aZPSCMPYg.png" medium="image"/><content:encoded><![CDATA[<h3 id="background">Background</h3><img src="https://blog.sparker0i.me/content/images/2020/04/1_ygv4NHtC9Jao-aZPSCMPYg.png" alt="Add new functions to existing classes the Scala way"><p>A <a href="https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/package.scala?ref=localhost#L46">Spark <code>DataFrame</code></a> has a better advantage over a <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html?ref=localhost">Pandas <code>DataFrame</code></a> when it comes to the ability to scale and process it. I&apos;m writing more on this in another blog post which will arrive shortly after this one. </p><p>Functionally, both Spark and Pandas have an almost same set of functionalities, and their APIs are not so different either. There&apos;s one function which is used extensively in the data science community with Pandas - <code><a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.shape.html?ref=localhost">shape()</a></code>. This function returns you the return the row and column count coupled inside a Tuple. Sadly, this functionality isn&apos;t available with Spark <code>DataFrame</code> (<a href="https://issues.apache.org/jira/browse/SPARK-27756?ref=localhost">and won&apos;t come either</a>).</p><h3 id="implicit-classes-in-scala">Implicit classes in Scala</h3><p>Fortunately, we have Implicit classes in Scala for our rescue. Implicit classes enable us to add some new functionality on top of an existing class&apos; functionalities. To know more about Implicit Classes, you can read <a href="http://www.lihaoyi.com/post/ImplicitDesignPatternsinScala.html?ref=localhost">this</a> article for diving deep. </p><p>First, we need to define a new implicit class with the method we want to add. In this case, I want to add the <code>shape()</code> function on top of the Spark <code>DataFrame</code> class.</p><pre><code class="language-scala">implicit class DataFramePlus(df: DataFrame) {
    def shape(): (Long, Int) = (df.count(), df.columns.length)
}</code></pre><p>Then all you need to do is print the shape of the <code>DataFrame</code>:</p><pre><code class="language-scala">df = spark.read.format(&quot;&lt;something&gt;&quot;).load(&quot;&lt;Filename&gt;&quot;)
println(df.shape())</code></pre><p>This solved a major pain point for me without having to extend an existing class.</p><h3 id="best-practice">Best Practice</h3><p>While writing these codes inside the Scala REPL (Scala/Spark Shell on Terminal) might seem a little easier to implement, openly exposing your code for everyone to use isn&apos;t a great idea.</p><p>Instead, you could implement the implicit class in a package object like this:</p><pre><code class="language-scala">package me.sparker0i

import org.apache.spark.sql.DataFrame

package object machinelearning {
    implicit class DataFramePlus(df: DataFrame) {
        def shape(): (Long, Int) = (df.count(), df.columns.length)
    }
}</code></pre><p>Then you&apos;ll need to add the proper import statement in your class, after which you can use the shape method with any <code>DataFrame</code>:</p><pre><code class="language-scala">package me.sparker0i.machinelearning.regression

import org.apache.spark.sql.DataFrame
import me.sparker0i.machinelearning._

class LinearRegression {
    def function(df: DataFrame): Unit = {
    	println(df.shape())
    }
}</code></pre><h3 id="conclusion">CONCLUSION</h3><p>With this approach of using implicit classes in Scala, we no longer have to extend any existing class just to add additional functionality to it. You define the behavior you want, and then add that behavior to existing class instances after adding the proper <code>import</code> statements.</p><p>Inspired heavily from <a href="https://alvinalexander.com/scala/scala-2.10-implicit-class-example/?ref=localhost">Alvin Alexander&apos;s article</a></p>]]></content:encoded></item><item><title><![CDATA[MySQL installation inside Docker on Mac]]></title><description><![CDATA[<!--kg-card-begin: html-->
<p>I&#x2019;ve never liked to make changes to my laptop by providing my admin credentials to install various tools, like MySQL because I love doing things at a user level. Programs like VSCode and Atom are good examples of that when all you do is drag the application icon</p>]]></description><link>https://blog.sparker0i.me/mysql-installation-inside-docker-on-mac/</link><guid isPermaLink="false">61f016ac0598f2d200c5e156</guid><category><![CDATA[Docker]]></category><category><![CDATA[mac]]></category><category><![CDATA[mysql]]></category><dc:creator><![CDATA[Aaditya Menon]]></dc:creator><pubDate>Thu, 28 Mar 2019 19:25:50 GMT</pubDate><media:content url="https://blog.sparker0i.me/content/images/wordpress/2019/03/1_fqwE_Sk2pCO5orZTRoivgg.png" medium="image"/><content:encoded><![CDATA[<!--kg-card-begin: html-->
<img src="https://blog.sparker0i.me/content/images/wordpress/2019/03/1_fqwE_Sk2pCO5orZTRoivgg.png" alt="MySQL installation inside Docker on Mac"><p>I&#x2019;ve never liked to make changes to my laptop by providing my admin credentials to install various tools, like MySQL because I love doing things at a user level. Programs like VSCode and Atom are good examples of that when all you do is drag the application icon to the Applications folder. Homebrew is also a good example to install command line applications at a user level.</p>



<p>If you were to install using the official installer, you&#x2019;d have to give administrator rights. You will also have to make changes to your bash profile to access mysql in terminal. All these problems can be alleviated by installing a MySQL container inside Docker.</p>



<blockquote class="wp-block-quote"><p>If you&#x2019;re using a Mac and want to use MySQL without making many installation changes, you&#x2019;ve come to the right place.  </p></blockquote>



<h3 id="install-docker">INSTALL DOCKER COMMUNITY EDITION</h3>



<p>Download Docker Desktop for Mac by visiting this <a href="https://hub.docker.com/search/?type=edition&amp;offering=community&amp;ref=localhost">link. </a>Then you need to install Docker on your Mac by following the installation steps. It is recommended not to change any defaults if prompted.</p>



<p>Once you are done with that, we will now proceed with the installation of MySQL container inside Docker. You can either do it inside a terminal, or use a tool like Kitematic using which you can manage multiple containers in your system once you create them. </p>



<p>Fire up a terminal, and write this command</p>



<pre class="wp-block-code"><code>docker run --name=mysql -d -p 3306:3306 -e MYSQL_USER=developer -e MYSQL_PASSWORD=mydbpwd -e MYSQL_DATABASE=mydb mysql/mysql-server</code></pre>



<p>Here we will use the <code>mysql/mysql-server</code> image, and our Username, password and Database are <code>developer</code>, <code>mydbpwd</code> and <code>mydb</code> respectively. Then do the port mapping between the container and the host. We bind Container&#x2019;s port 3306 to the Mac&#x2019;s port 3306.</p>



<p>If everything goes fine, you should see a combination of alphabets and numbers as an output. This is the container ID. Type this command:</p>



<pre class="wp-block-code"><code>docker exec -it mysql bash -c &quot;mysql -u developer -p&quot;</code></pre>



<p>Then enter the password you entered while creating the container (In this case <code>mydbpwd</code>). Then you will have an instance up and running inside Docker.</p>



<h3>CHECK MYSQL CONNECTIVITY INSIDE MYSQL WORKBENCH</h3>



<p>First up install MySQL Workbench, either using the official installer or using the brew command <code>brew cask install mysqlworkbench</code>. Once you open up the MySQLWorkbench, click on the add connection button, then enter as following:</p>



<div class="wp-block-image"><figure class="aligncenter"><img src="https://blog.sparker0i.me/content/images/wordpress/2019/03/image.png" alt="MySQL installation inside Docker on Mac" class="wp-image-678" srcset="https://blog.sparker0i.me/content/images/wordpress/2019/03/image.png 798w, https://blog.sparker0i.me/content/images/wordpress/2019/03/image-300x188.png 300w, https://blog.sparker0i.me/content/images/wordpress/2019/03/image-768x481.png 768w" sizes="(max-width: 798px) 100vw, 798px"><figcaption>Creating a new connection inside MySQL Workbench<br></figcaption></figure></div>



<p>Then click the Test Connection button. If everything goes alright, you will get this popup:</p>



<div class="wp-block-image"><figure class="aligncenter"><img src="https://blog.sparker0i.me/content/images/wordpress/2019/03/image-1.png" alt="MySQL installation inside Docker on Mac" class="wp-image-679" srcset="https://blog.sparker0i.me/content/images/wordpress/2019/03/image-1.png 420w, https://blog.sparker0i.me/content/images/wordpress/2019/03/image-1-300x169.png 300w" sizes="(max-width: 420px) 100vw, 420px"><figcaption>Connection established with MySQL Container in Docker</figcaption></figure></div>



<h3>CONCLUSION</h3>



<p>Installing MySQL in Docker on your PC is a safer approach to installing MySQL than providing Admin credentials to install using the official installer. If anything goes wrong, all you have to do is delete the container and create a new one in Docker.</p>



<h3>REFERENCES</h3>



<p><a href="https://medium.com/@crmcmullen/how-to-run-mysql-in-a-docker-container-on-macos-with-persistent-local-data-58b89aec496a?ref=localhost">Run MySQL in a Docker Container &#x2013; Medium</a></p>
<!--kg-card-end: html-->]]></content:encoded></item><item><title><![CDATA[Announcing Simple Weather v4]]></title><description><![CDATA[<!--kg-card-begin: html--><p>On the eve of my birthday, here&#x2019;s a surprise &#x2013; a big update. Welcome to Simple Weather v4. This is a really big release. I&#x2019;ve heard from the Open Source Community (FDroid , Emails , GitHub issues) and the Play Store Reviews. They all wanted me to add</p>]]></description><link>https://blog.sparker0i.me/announcing-simple-weather-v4/</link><guid isPermaLink="false">61f016ac0598f2d200c5e14a</guid><category><![CDATA[Android]]></category><category><![CDATA[Java]]></category><category><![CDATA[Programming]]></category><category><![CDATA[Simple Weather]]></category><category><![CDATA[Workshop]]></category><dc:creator><![CDATA[Aaditya Menon]]></dc:creator><pubDate>Sun, 04 Jun 2017 00:52:08 GMT</pubDate><media:content url="https://blog.sparker0i.me/content/images/wordpress/2017/11/weather2-1.png" medium="image"/><content:encoded><![CDATA[<!--kg-card-begin: html--><img src="https://blog.sparker0i.me/content/images/wordpress/2017/11/weather2-1.png" alt="Announcing Simple Weather v4"><p>On the eve of my birthday, here&#x2019;s a surprise &#x2013; a big update. Welcome to Simple Weather v4. This is a really big release. I&#x2019;ve heard from the Open Source Community (FDroid , Emails , GitHub issues) and the Play Store Reviews. They all wanted me to add a few new features to the already wonderful app. So without wasting time, I&#x2019;ll tour you through the new features.</p>
<p></p>
<h3>NEW FEATURES</h3>
<ul>
<li>New weather Screen with minor tweaks</li>
<li>Weather Widget and Hourly Notifications</li>
<li>New Weather Graphs and Weather Maps</li>
<li>View Weather Data in Fahrenheit too</li>
<li>Use your own custom Open Weather Map key</li>
<li>Refreshed About Screen</li>
<li>New Icon</li>
</ul>
<p></p>
<h3>DOWNLOAD THE APP</h3>
<p>With this release, you now have two sources to download the app from &#x2013; the Google Play Store and the F Droid Store. While this update is currently live on the Play Store, it will take some time to arrive on FDroid Store (PS. Do not download v3 from the FDroid store, it was meant for Beta testing) (because FDroid isn&#x2019;t that great when it comes to scanning through releases, v4 will soon be available there in a few days).</p>
<p><a href="https://f-droid.org/repository/browse/?fdid=com.a5corp.weather&amp;ref=localhost">F-Droid</a><br>
<a href="https://play.google.com/store/apps/details?id=com.a5corp.weather&amp;ref=localhost"><img src="https://play.google.com/intl/en_us/badges/images/generic/en_badge_web_generic.png" style="width: 210px; height: 80px;" alt="Announcing Simple Weather v4"></a></p>
<p></p>
<h3>NEW ICON</h3>
<p>Ah, sadly yes, this is one of the changes too. I know I had just changed the icon in v2, but I think this was really necessary to convey what my app is trying to deliver. Sadly a Sun over a black background would mean complete darkness. So this was necessary. It is also aligned to the main color of my app. Enjoy the new icon.</p>
<p><a href="https://blog.sparker0i.me/content/images/wordpress/2017/11/1-1.png"><img src="https://blog.sparker0i.me/content/images/wordpress/2017/11/1-1.png" alt="Announcing Simple Weather v4" width="2134" height="3840" class="alignnone size-full wp-image-174" srcset="https://blog.sparker0i.me/content/images/wordpress/2017/11/1-1.png 2134w, https://blog.sparker0i.me/content/images/wordpress/2017/11/1-1-167x300.png 167w, https://blog.sparker0i.me/content/images/wordpress/2017/11/1-1-768x1382.png 768w, https://blog.sparker0i.me/content/images/wordpress/2017/11/1-1-569x1024.png 569w" sizes="(max-width: 2134px) 100vw, 2134px"></a></p>
<p></p>
<h3>SLIGHTLY TWEAKED WEATHER SCREEN, NEW WEATHER OPTIONS</h3>
<p>Once you upgrade to this new version, you will now see a slightly tweaked main screen. All the information you need, is now in one place. Right here,</p>
<p><a href="https://blog.sparker0i.me/content/images/wordpress/2017/11/2-1.png"><img src="https://blog.sparker0i.me/content/images/wordpress/2017/11/2-1.png" alt="Announcing Simple Weather v4" width="2134" height="3840" class="alignnone size-full wp-image-175" srcset="https://blog.sparker0i.me/content/images/wordpress/2017/11/2-1.png 2134w, https://blog.sparker0i.me/content/images/wordpress/2017/11/2-1-167x300.png 167w, https://blog.sparker0i.me/content/images/wordpress/2017/11/2-1-768x1382.png 768w, https://blog.sparker0i.me/content/images/wordpress/2017/11/2-1-569x1024.png 569w" sizes="(max-width: 2134px) 100vw, 2134px"></a></p>
<p>As you can also see from the picture above, the Search Red Floating Button has now been moved to the toolbar. This was necessary because in lot of the Reviews I had seen that the FAB (floating Button) usually would hide the details of the last weather item. I could not find a replacement in time, thus I had to move the search button to the main toolbar. And also you can see a Drawer toggle in the toolbar, Right? Yay, open that up and you see this:</p>
<p><a href="https://blog.sparker0i.me/content/images/wordpress/2017/11/3-1.png"><img src="https://blog.sparker0i.me/content/images/wordpress/2017/11/3-1.png" alt="Announcing Simple Weather v4" width="2134" height="3840" class="alignnone size-full wp-image-176" srcset="https://blog.sparker0i.me/content/images/wordpress/2017/11/3-1.png 2134w, https://blog.sparker0i.me/content/images/wordpress/2017/11/3-1-167x300.png 167w, https://blog.sparker0i.me/content/images/wordpress/2017/11/3-1-768x1382.png 768w, https://blog.sparker0i.me/content/images/wordpress/2017/11/3-1-569x1024.png 569w" sizes="(max-width: 2134px) 100vw, 2134px"></a></p>
<p>This upgrade adds a big request you have all been asking for, Fahrenheit temperatures. Hit the toggle, and the weather screen (if visible) will show you the new Fahrenheit values. I know this could have been better (rather than always refreshing when hitting that toggle), but I will work on it when I get time .</p>
<p>You can also see 2 new options under the Home tab &#x2013; Weather Graphs and Weather Maps. These are 2 new ways to display the weather information:</p>
<p><a href="https://blog.sparker0i.me/content/images/wordpress/2017/11/4-1.png"><img class="alignnone size-full wp-image-177" src="https://blog.sparker0i.me/content/images/wordpress/2017/11/4-1.png" alt="Announcing Simple Weather v4" width="2134" height="3840"></a></p>
<div id="slider_183_slide02" class="sa_hover_container" style="padding:5% 5%; margin:0px 0%; "><p><a href="https://blog.sparker0i.me/content/images/wordpress/2017/11/5-1.png"><img class="alignnone size-full wp-image-178" src="https://blog.sparker0i.me/content/images/wordpress/2017/11/5-1.png" alt="Announcing Simple Weather v4" width="2134" height="3840"></a></p>

<p>What&#x2019;s more exciting is that you can launch this from your home screen right away. So you need not visit the app ,open the drawer and select the option. Rather, if you are running Android 7.1.1+, you could do this from your launcher as well.</p>
<p><a href="https://blog.sparker0i.me/content/images/wordpress/2017/11/6.png"><img src="https://blog.sparker0i.me/content/images/wordpress/2017/11/6.png" alt="Announcing Simple Weather v4" width="2134" height="3840" class="alignnone size-full wp-image-179" srcset="https://blog.sparker0i.me/content/images/wordpress/2017/11/6.png 2134w, https://blog.sparker0i.me/content/images/wordpress/2017/11/6-167x300.png 167w, https://blog.sparker0i.me/content/images/wordpress/2017/11/6-768x1382.png 768w, https://blog.sparker0i.me/content/images/wordpress/2017/11/6-569x1024.png 569w" sizes="(max-width: 2134px) 100vw, 2134px"></a></p>
<p>On the Weather Graphs screen, if you hit that empty circle on top, it will show the values on the graph. Moreover, you can change between the three states (Rain, Wind, Temperature) in the Weather Maps screen too.</p>
<p></p>
<h3>WEATHER NOTIFICATIONS</h3>
<p>You can also opt to see weather notifications on an hourly basis as well. Just hit the &#x201C;Show Hourly Notifications&#x201D; toggle, and you will get essential weather updates every hour, like this:</p>
<p><a href="https://blog.sparker0i.me/content/images/wordpress/2017/11/7.png"><img src="https://blog.sparker0i.me/content/images/wordpress/2017/11/7.png" alt="Announcing Simple Weather v4" width="2134" height="3840" class="alignnone size-full wp-image-180" srcset="https://blog.sparker0i.me/content/images/wordpress/2017/11/7.png 2134w, https://blog.sparker0i.me/content/images/wordpress/2017/11/7-167x300.png 167w, https://blog.sparker0i.me/content/images/wordpress/2017/11/7-768x1382.png 768w, https://blog.sparker0i.me/content/images/wordpress/2017/11/7-569x1024.png 569w" sizes="(max-width: 2134px) 100vw, 2134px"></a></p>
<p></p>
<h3>WEATHER WIDGETS</h3>
<p>Here is another big feature request being released today &#x2013; Weather Widgets. These might be slightly buggy on your phone restart (I will work on this in the near future too), otherwise these should just run fine. There are 2 sizes of this widget &#x2013; Large and Small, whose size you can increase if your launcher supports that option.</p>
<p><a href="https://blog.sparker0i.me/content/images/wordpress/2017/11/8.png"><img src="https://blog.sparker0i.me/content/images/wordpress/2017/11/8-768x1382.png" alt="Announcing Simple Weather v4" style="width: 1024px; height: 569px;"></a></p>
<p></p>
<h3>CUSTOM OPENWEATHERMAP KEY</h3>
<p>As you can see in the Drawer pic, you can now choose to load the Weather Data with your own Custom OWM Key (I would encourage, not recommend all to perform this daring step). Doing so can help slow down the amount of server crashes happening due to my Key.</p>
<p></p>
<h3>A NEW ABOUT SCREEN</h3>
<p>Also, here is a slightly tweaked About Screen</p>
<p><a href="https://blog.sparker0i.me/content/images/wordpress/2017/11/9.png"><img src="https://blog.sparker0i.me/content/images/wordpress/2017/11/9.png" alt="Announcing Simple Weather v4" width="2134" height="3840" class="alignnone size-full wp-image-182" srcset="https://blog.sparker0i.me/content/images/wordpress/2017/11/9.png 2134w, https://blog.sparker0i.me/content/images/wordpress/2017/11/9-167x300.png 167w, https://blog.sparker0i.me/content/images/wordpress/2017/11/9-768x1382.png 768w, https://blog.sparker0i.me/content/images/wordpress/2017/11/9-569x1024.png 569w" sizes="(max-width: 2134px) 100vw, 2134px"></a></p>
<p></p>
<h4>WHY IS THE APP v4 AND NOT v3</h4>
<p>Potentially, this question would have arose on seeing the title itself, &#x201C;Where did v3 go?&#x201D;. It never went anywhere, I had decided to do Beta testing with v3. Another reason why I didn&#x2019;t name the Play Store release as v3 was because the number 3 is unlucky for me. In my early school days, I was always ranked #3 , which never excited me. I started to feel #3 was plain and boring right from then (and unlucky too).</p>
<!--kg-card-end: html--></div>]]></content:encoded></item><item><title><![CDATA[Remove Ubuntu Bootloader code from Windows 10]]></title><description><![CDATA[Ever uninstalled Ubuntu from your machine but the GRUB still remains? Know how to remove Ubuntu completely from your machine.]]></description><link>https://blog.sparker0i.me/remove-linux-bootloader-code-from-windows/</link><guid isPermaLink="false">61f016ac0598f2d200c5e147</guid><category><![CDATA[Linux]]></category><category><![CDATA[Programming]]></category><category><![CDATA[Python]]></category><category><![CDATA[Solving-Technical]]></category><category><![CDATA[Ubuntu]]></category><dc:creator><![CDATA[Aaditya Menon]]></dc:creator><pubDate>Tue, 07 Feb 2017 18:59:25 GMT</pubDate><media:content url="https://blog.sparker0i.me/content/images/wordpress/2017/11/ubuntu.png" medium="image"/><content:encoded><![CDATA[<!--kg-card-begin: html--><img src="https://blog.sparker0i.me/content/images/wordpress/2017/11/ubuntu.png" alt="Remove Ubuntu Bootloader code from Windows 10"><p>Hello folks, I am back with yet another post in the Solving Technical Series. In certain situations, when a program you want to run does not work properly on an OS, you first blame the program. Then you try to run it on another OS, and find it working without issues. Then you go back and blame the OS. This is what happens to me when I try to run the Android Studio Emulator on Linux. Its screen size remains small despite resizing. Resizing works fine for Windows 10.</p>
<p>As I am a part of the FOSS Club of my college, I know that I must be embracing Open Source Software, but in certain situations like these, my head just gets fried up.</p>
<p>So one fine day, I decided to remove Linux from my PC. I removed those Linux partitions, but unfortunately the Ubuntu Bootloader (aka GRUB) was still present on the Boot screen. Then I had to find a way to remove it from Windows. And voila, there is a way out.</p>
<p></p>
<h3>STEPS TO BE FOLLOWED</h3>
<ul>
<li>Open Command Prompt as an Administrator.<p></p>
</li>
<li>
<p>Now enter <code>diskpart</code> on the CMD screen</p>
</li>
</ul>
<p><a href="https://blog.sparker0i.me/content/images/wordpress/2017/11/diskpart.png"><img src="https://blog.sparker0i.me/content/images/wordpress/2017/11/diskpart.png" alt="Remove Ubuntu Bootloader code from Windows 10" width="856" height="450" class="alignnone size-full wp-image-139" srcset="https://blog.sparker0i.me/content/images/wordpress/2017/11/diskpart.png 856w, https://blog.sparker0i.me/content/images/wordpress/2017/11/diskpart-300x158.png 300w, https://blog.sparker0i.me/content/images/wordpress/2017/11/diskpart-768x404.png 768w" sizes="(max-width: 856px) 100vw, 856px"></a></p>
<ul>
<li>Now enter <code>list disk</code>. You will see your Hard Disk and the external storage drives connected to your PC. Select the PC Hard Disk Number. It should be 0 by default on all the systems. But still, check your hardware and then do.</li>
</ul>
<p><a href="https://blog.sparker0i.me/content/images/wordpress/2017/11/list-disk.png"><img src="https://blog.sparker0i.me/content/images/wordpress/2017/11/list-disk.png" alt="Remove Ubuntu Bootloader code from Windows 10" width="853" height="451" class="alignnone size-full wp-image-140" srcset="https://blog.sparker0i.me/content/images/wordpress/2017/11/list-disk.png 853w, https://blog.sparker0i.me/content/images/wordpress/2017/11/list-disk-300x159.png 300w, https://blog.sparker0i.me/content/images/wordpress/2017/11/list-disk-768x406.png 768w" sizes="(max-width: 853px) 100vw, 853px"></a></p>
<ul>
<li>Select Disk 0 (or whatever number it is of your HDD) by typing <code>sel disk 0</code>. Now to check a list of partitions in it, type <code>list vol</code></li>
</ul>
<p><a href="https://blog.sparker0i.me/content/images/wordpress/2017/11/list-vol.png"><img src="https://blog.sparker0i.me/content/images/wordpress/2017/11/list-vol.png" alt="Remove Ubuntu Bootloader code from Windows 10" width="860" height="451" class="alignnone size-full wp-image-144" srcset="https://blog.sparker0i.me/content/images/wordpress/2017/11/list-vol.png 860w, https://blog.sparker0i.me/content/images/wordpress/2017/11/list-vol-300x157.png 300w, https://blog.sparker0i.me/content/images/wordpress/2017/11/list-vol-768x403.png 768w" sizes="(max-width: 860px) 100vw, 860px"></a></p>
<ul>
<li>On 64 bit systems, it should be a 100 MB FAT32 system that has &#x2018;System&#x2019; as the Info in it. Note the volume number. In my case, it is 9</li>
</ul>
<p><a href="https://blog.sparker0i.me/content/images/wordpress/2017/11/diskpart.png"><img src="https://blog.sparker0i.me/content/images/wordpress/2017/11/diskpart.png" alt="Remove Ubuntu Bootloader code from Windows 10" width="856" height="450" class="alignnone size-full wp-image-139" srcset="https://blog.sparker0i.me/content/images/wordpress/2017/11/diskpart.png 856w, https://blog.sparker0i.me/content/images/wordpress/2017/11/diskpart-300x158.png 300w, https://blog.sparker0i.me/content/images/wordpress/2017/11/diskpart-768x404.png 768w" sizes="(max-width: 856px) 100vw, 856px"></a></p>
<ul>
<li>Type <code>sel vol 9</code>.</li>
</ul>
<p><a href="https://blog.sparker0i.me/content/images/wordpress/2017/11/sel-vol.png"><img src="https://blog.sparker0i.me/content/images/wordpress/2017/11/sel-vol.png" alt="Remove Ubuntu Bootloader code from Windows 10" width="858" height="453" class="alignnone size-full wp-image-146" srcset="https://blog.sparker0i.me/content/images/wordpress/2017/11/sel-vol.png 858w, https://blog.sparker0i.me/content/images/wordpress/2017/11/sel-vol-300x158.png 300w, https://blog.sparker0i.me/content/images/wordpress/2017/11/sel-vol-768x405.png 768w" sizes="(max-width: 858px) 100vw, 858px"></a></p>
<ul>
<li>Now this volume has been selected. We now have to assign the disk letter (for which Windows/MS-DOS became famous for). I assigned a letter Z, by typing <code>assign letter=Z:</code>. Then <code>exit</code> diskpart.<p></p>
</li>
<li>
<p>Now you are back to your regular CMD Prompt. Now type <code>cd /d Z:</code>. Note the <code>/d</code> arguement.</p>
</li>
</ul>
<p><a href="https://blog.sparker0i.me/content/images/wordpress/2017/11/cd-Z.png"><img src="https://blog.sparker0i.me/content/images/wordpress/2017/11/cd-Z.png" alt="Remove Ubuntu Bootloader code from Windows 10" width="858" height="454" class="alignnone size-full wp-image-141" srcset="https://blog.sparker0i.me/content/images/wordpress/2017/11/cd-Z.png 858w, https://blog.sparker0i.me/content/images/wordpress/2017/11/cd-Z-300x159.png 300w, https://blog.sparker0i.me/content/images/wordpress/2017/11/cd-Z-768x406.png 768w" sizes="(max-width: 858px) 100vw, 858px"></a></p>
<ul>
<li>Now for this step, there are two ways to approach this. Some systems will have the <code>ls</code> command, while some won&#x2019;t. While I would recommend using the <code>ls</code> command, for those who don&#x2019;t have it can use the <code>dir</code> command as well. Type <code>ls</code> or <code>dir</code>, depending on the program you have</li>
</ul>
<p><a href="https://blog.sparker0i.me/content/images/wordpress/2017/11/dir.png"><img src="https://blog.sparker0i.me/content/images/wordpress/2017/11/dir.png" alt="Remove Ubuntu Bootloader code from Windows 10" width="856" height="449" class="alignnone size-full wp-image-142" srcset="https://blog.sparker0i.me/content/images/wordpress/2017/11/dir.png 856w, https://blog.sparker0i.me/content/images/wordpress/2017/11/dir-300x157.png 300w, https://blog.sparker0i.me/content/images/wordpress/2017/11/dir-768x403.png 768w" sizes="(max-width: 856px) 100vw, 856px"></a></p>
<ul>
<li><code>cd</code> into the EFI folder. Then type <code>ls</code> again. You will now see a <code>Ubuntu</code> option.</li>
</ul>
<p><a href="https://blog.sparker0i.me/content/images/wordpress/2017/11/dir2.png"><img src="https://blog.sparker0i.me/content/images/wordpress/2017/11/dir2.png" alt="Remove Ubuntu Bootloader code from Windows 10" width="517" height="164" class="alignnone size-full wp-image-143" srcset="https://blog.sparker0i.me/content/images/wordpress/2017/11/dir2.png 517w, https://blog.sparker0i.me/content/images/wordpress/2017/11/dir2-300x95.png 300w" sizes="(max-width: 517px) 100vw, 517px"></a></p>
<ul>
<li>To remove this Ubuntu GRUB, type <code>rmdir /s Ubuntu</code>. It will ask for a confirmation Prompt, type <code>Y</code> and press Enter.</li>
</ul>
<p><a href="https://blog.sparker0i.me/content/images/wordpress/2017/11/rmdir.png"><img src="https://blog.sparker0i.me/content/images/wordpress/2017/11/rmdir.png" alt="Remove Ubuntu Bootloader code from Windows 10" width="309" height="78" class="alignnone size-full wp-image-145" srcset="https://blog.sparker0i.me/content/images/wordpress/2017/11/rmdir.png 309w, https://blog.sparker0i.me/content/images/wordpress/2017/11/rmdir-300x76.png 300w" sizes="(max-width: 309px) 100vw, 309px"></a></p>
<ul>
<li>Voila! Your PC won&#x2019;t have the Ubuntu Bootloader anymore.</li>
</ul>
<p></p>
<h3>CONCLUSION</h3>
<p>Sometimes, at any given point of time, any given OS can be frustrating. Choose your OS depending on the features you need.</p>
<p></p>
<h3>UPDATE</h3>
<p>Android Studio 2.3 Update on Linux fixes the issue I had. So I am back on Linux. Hell Yeah!</p>
<!--kg-card-end: html-->]]></content:encoded></item><item><title><![CDATA[Announcing Simple Weather v2.0]]></title><description><![CDATA[<!--kg-card-begin: html--><p>Hello there. Wishing all a Merry Christmas and a Happy new year. Today I&#x2019;m writing this blog post to announce the second release of my Simple Weather app. The app was started out as an assignment to the CS With Android Workshop that was held in our college</p>]]></description><link>https://blog.sparker0i.me/simple-weather-v2-now-available/</link><guid isPermaLink="false">61f016ac0598f2d200c5e146</guid><category><![CDATA[Android]]></category><category><![CDATA[Programming]]></category><category><![CDATA[Simple Weather]]></category><dc:creator><![CDATA[Aaditya Menon]]></dc:creator><pubDate>Mon, 12 Dec 2016 10:10:31 GMT</pubDate><media:content url="https://blog.sparker0i.me/content/images/wordpress/2017/11/weather2.png" medium="image"/><content:encoded><![CDATA[<!--kg-card-begin: html--><img src="https://blog.sparker0i.me/content/images/wordpress/2017/11/weather2.png" alt="Announcing Simple Weather v2.0"><p>Hello there. Wishing all a Merry Christmas and a Happy new year. Today I&#x2019;m writing this blog post to announce the second release of my Simple Weather app. The app was started out as an assignment to the CS With Android Workshop that was held in our college last summer. The app had an intuitive and beautiful UI.</p>
<p>This second release carries on the same UI from the previous release, with improvements to the app overall. Before I start, here is a stat on the number of downloads till date (until the moment I pushed the app update).</p>
<p><a href="https://blog.sparker0i.me/content/images/wordpress/2017/11/stat.png"><img src="https://blog.sparker0i.me/content/images/wordpress/2017/11/stat-576x1024.png" alt="Announcing Simple Weather v2.0" width="576" height="1024" class="alignnone size-large wp-image-122" srcset="https://blog.sparker0i.me/content/images/wordpress/2017/11/stat-576x1024.png 576w, https://blog.sparker0i.me/content/images/wordpress/2017/11/stat-169x300.png 169w, https://blog.sparker0i.me/content/images/wordpress/2017/11/stat-768x1365.png 768w, https://blog.sparker0i.me/content/images/wordpress/2017/11/stat.png 1080w" sizes="(max-width: 576px) 100vw, 576px"></a></p>
<p>Okay, starting off, you will see a new icon in the launcher. I have decided to take a Circular Icon approach for my icon for all versions of Android (unlike Android 7.1 where this is offered as a roundIcon option) :</p>
<p><a href="https://blog.sparker0i.me/content/images/wordpress/2017/11/icon.png"><img src="https://blog.sparker0i.me/content/images/wordpress/2017/11/icon-569x1024.png" alt="Announcing Simple Weather v2.0" width="569" height="1024" class="alignnone size-large wp-image-123" srcset="https://blog.sparker0i.me/content/images/wordpress/2017/11/icon-569x1024.png 569w, https://blog.sparker0i.me/content/images/wordpress/2017/11/icon-167x300.png 167w, https://blog.sparker0i.me/content/images/wordpress/2017/11/icon-768x1382.png 768w, https://blog.sparker0i.me/content/images/wordpress/2017/11/icon.png 1600w" sizes="(max-width: 569px) 100vw, 569px"></a></p>
<p>When you start the app, the first thing you will notice is change of app colors (Blue-Red, unlike Purple-Pink from the previous release) and a floating action bar icon for opening up the search box.</p>
<p><a href="https://blog.sparker0i.me/content/images/wordpress/2017/11/home.png"><img src="https://blog.sparker0i.me/content/images/wordpress/2017/11/home-569x1024.png" alt="Announcing Simple Weather v2.0" width="569" height="1024" class="alignnone size-large wp-image-124" srcset="https://blog.sparker0i.me/content/images/wordpress/2017/11/home-569x1024.png 569w, https://blog.sparker0i.me/content/images/wordpress/2017/11/home-167x300.png 167w, https://blog.sparker0i.me/content/images/wordpress/2017/11/home-768x1382.png 768w, https://blog.sparker0i.me/content/images/wordpress/2017/11/home.png 1600w" sizes="(max-width: 569px) 100vw, 569px"></a></p>
<p>In the previous update of the app, whenever you clicked on the City Name or the Weather Icon, the app used to display a Dialog Box showing information in an unsorted way &#x2013; differently on different devices. This thing has changed in v2.0 . Now when you click on either of them, you will be shown a Snackbar, which tends to be a more modern, &#x201C;Material&#x201D;istic approach to show information.</p>
<p><a href="https://blog.sparker0i.me/content/images/wordpress/2017/11/snack.png"><img src="https://blog.sparker0i.me/content/images/wordpress/2017/11/snack-569x1024.png" alt="Announcing Simple Weather v2.0" width="569" height="1024" class="alignnone size-large wp-image-125" srcset="https://blog.sparker0i.me/content/images/wordpress/2017/11/snack-569x1024.png 569w, https://blog.sparker0i.me/content/images/wordpress/2017/11/snack-167x300.png 167w, https://blog.sparker0i.me/content/images/wordpress/2017/11/snack-768x1382.png 768w, https://blog.sparker0i.me/content/images/wordpress/2017/11/snack.png 1600w" sizes="(max-width: 569px) 100vw, 569px"></a></p>
<p>In a similar fashion, when you clicked to display a Day&#x2019;s information, you would be presented with a dialog box, which had rendering problems, just like the one I had mentioned above. This thing is also changing with v2.0 . Now when you click to display a day&#x2019;s information, you will be taken to a new screen which shows information as icons and texts. Click on any one of them to reveal what that icon/text is for. This will also be shown as a Snackbar.</p>
<p><a href="https://blog.sparker0i.me/content/images/wordpress/2017/11/detail.png"><img src="https://blog.sparker0i.me/content/images/wordpress/2017/11/detail-569x1024.png" alt="Announcing Simple Weather v2.0" width="569" height="1024" class="alignnone size-large wp-image-126" srcset="https://blog.sparker0i.me/content/images/wordpress/2017/11/detail-569x1024.png 569w, https://blog.sparker0i.me/content/images/wordpress/2017/11/detail-167x300.png 167w, https://blog.sparker0i.me/content/images/wordpress/2017/11/detail-768x1382.png 768w, https://blog.sparker0i.me/content/images/wordpress/2017/11/detail.png 1600w" sizes="(max-width: 569px) 100vw, 569px"></a></p>
<p>Another thing you&#x2019;ll notice in the main app is that icon near the 3 dot menu. That is the location icon. Click on it to detect your current location, and the app will display data accordingly.</p>
<p><a href="https://blog.sparker0i.me/content/images/wordpress/2017/11/location.png"><img src="https://blog.sparker0i.me/content/images/wordpress/2017/11/location-569x1024.png" alt="Announcing Simple Weather v2.0" width="569" height="1024" class="alignnone size-large wp-image-127" srcset="https://blog.sparker0i.me/content/images/wordpress/2017/11/location-569x1024.png 569w, https://blog.sparker0i.me/content/images/wordpress/2017/11/location-167x300.png 167w, https://blog.sparker0i.me/content/images/wordpress/2017/11/location-768x1382.png 768w, https://blog.sparker0i.me/content/images/wordpress/2017/11/location.png 1600w" sizes="(max-width: 569px) 100vw, 569px"></a></p>
<p>If you are using Android 6.0 and above, you&#x2019;ll be presented with a dialog box to enable location permission. If you click on that location icon on the action bar, you will need to accept the permission for the action to continue.</p>
<p><a href="https://blog.sparker0i.me/content/images/wordpress/2017/11/permission.png"><img src="https://blog.sparker0i.me/content/images/wordpress/2017/11/permission-569x1024.png" alt="Announcing Simple Weather v2.0" width="569" height="1024" class="alignnone size-large wp-image-128" srcset="https://blog.sparker0i.me/content/images/wordpress/2017/11/permission-569x1024.png 569w, https://blog.sparker0i.me/content/images/wordpress/2017/11/permission-167x300.png 167w, https://blog.sparker0i.me/content/images/wordpress/2017/11/permission-768x1382.png 768w, https://blog.sparker0i.me/content/images/wordpress/2017/11/permission.png 1600w" sizes="(max-width: 569px) 100vw, 569px"></a></p>
<p>There&#x2019;s one more new thing in the app. You now have the ability to search for a city based on its ZIP Code, or what we call in India as the PIN Code. That is what we call an improvement to an existing thing.</p>
<p><a href="https://blog.sparker0i.me/content/images/wordpress/2017/11/search.png"><img src="https://blog.sparker0i.me/content/images/wordpress/2017/11/search-569x1024.png" alt="Announcing Simple Weather v2.0" width="569" height="1024" class="alignnone size-large wp-image-129" srcset="https://blog.sparker0i.me/content/images/wordpress/2017/11/search-569x1024.png 569w, https://blog.sparker0i.me/content/images/wordpress/2017/11/search-167x300.png 167w, https://blog.sparker0i.me/content/images/wordpress/2017/11/search-768x1382.png 768w, https://blog.sparker0i.me/content/images/wordpress/2017/11/search.png 1600w" sizes="(max-width: 569px) 100vw, 569px"></a></p>
<p>The app is now rolling all over the globe. Make sure that you download the app today.</p>
<p><a href="https://play.google.com/store/apps/details?id=com.a5corp.weather&amp;utm_source=global_co&amp;utm_medium=prtnr&amp;utm_content=Mar2515&amp;utm_campaign=PartBadge&amp;pcampaignid=MKT-Other-global-all-co-prtnr-py-PartBadge-Mar2515-1"><img alt="Announcing Simple Weather v2.0" src="https://play.google.com/intl/en_us/badges/images/generic/en_badge_web_generic.png"></a></p>
<p><a href="https://blog.sparker0i.me/content/images/wordpress/2017/11/about.png"><img src="https://blog.sparker0i.me/content/images/wordpress/2017/11/about-569x1024.png" alt="Announcing Simple Weather v2.0" width="569" height="1024" class="alignnone size-large wp-image-130" srcset="https://blog.sparker0i.me/content/images/wordpress/2017/11/about-569x1024.png 569w, https://blog.sparker0i.me/content/images/wordpress/2017/11/about-167x300.png 167w, https://blog.sparker0i.me/content/images/wordpress/2017/11/about-768x1382.png 768w, https://blog.sparker0i.me/content/images/wordpress/2017/11/about.png 1600w" sizes="(max-width: 569px) 100vw, 569px"></a></p>
<p>Until another blog post, Ciao.</p>
<!--kg-card-end: html-->]]></content:encoded></item></channel></rss>